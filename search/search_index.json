{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"gridfm-graphkit","text":"<p>This library is brought to you by the GridFM team to train, finetune and interact with a foundation model for the electric power grid.</p> <p> </p>"},{"location":"#citation-tbd","title":"Citation: TBD","text":""},{"location":"datasets/data_normalization/","title":"Data Normalization","text":"<p>Normalization improves neural network training by ensuring features are well-scaled, preventing issues like exploding gradients and slow convergence. In power grids, where variables like voltage and power span wide ranges, normalization is essential. The <code>gridfm-graphkit</code> package offers four methods:</p> <ul> <li><code>Min-Max Normalization</code></li> <li><code>Standardization (Z-score)</code></li> <li><code>Identity (no normalization)</code></li> <li><code>BaseMVA Normalization</code></li> </ul> <p>Each of these strategies implements a unified interface and can be used interchangeably depending on the learning task and data characteristics.</p> <p>Users can create their own custom normalizers by extending the base <code>Normalizer</code> class to suit specific needs.</p>"},{"location":"datasets/data_normalization/#available-normalizers","title":"Available Normalizers","text":""},{"location":"datasets/data_normalization/#normalizer","title":"<code>Normalizer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all normalization strategies.</p> Source code in <code>gridfm_graphkit/datasets/data_normalization.py</code> <pre><code>class Normalizer(ABC):\n    \"\"\"\n    Abstract base class for all normalization strategies.\n    \"\"\"\n\n    @abstractmethod\n    def fit(self, data: torch.Tensor) -&gt; dict:\n        \"\"\"\n        Fit normalization parameters from data.\n\n        Args:\n            data: Input tensor.\n\n        Returns:\n            Dictionary of computed parameters.\n        \"\"\"\n\n    @abstractmethod\n    def fit_from_dict(self, params: dict):\n        \"\"\"\n        Set parameters from a precomputed dictionary.\n\n        Args:\n            params: Dictionary of parameters.\n        \"\"\"\n\n    @abstractmethod\n    def transform(self, data: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Normalize the input data.\n\n        Args:\n            data: Input tensor.\n\n        Returns:\n            Normalized tensor.\n        \"\"\"\n\n    @abstractmethod\n    def inverse_transform(self, normalized_data: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Undo normalization.\n\n        Args:\n            normalized_data: Normalized tensor.\n\n        Returns:\n            Original tensor.\n        \"\"\"\n</code></pre> <code>fit(data)</code> <code>abstractmethod</code> \u00b6 <p>Fit normalization parameters from data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of computed parameters.</p> Source code in <code>gridfm_graphkit/datasets/data_normalization.py</code> <pre><code>@abstractmethod\ndef fit(self, data: torch.Tensor) -&gt; dict:\n    \"\"\"\n    Fit normalization parameters from data.\n\n    Args:\n        data: Input tensor.\n\n    Returns:\n        Dictionary of computed parameters.\n    \"\"\"\n</code></pre> <code>fit_from_dict(params)</code> <code>abstractmethod</code> \u00b6 <p>Set parameters from a precomputed dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Dictionary of parameters.</p> required Source code in <code>gridfm_graphkit/datasets/data_normalization.py</code> <pre><code>@abstractmethod\ndef fit_from_dict(self, params: dict):\n    \"\"\"\n    Set parameters from a precomputed dictionary.\n\n    Args:\n        params: Dictionary of parameters.\n    \"\"\"\n</code></pre> <code>inverse_transform(normalized_data)</code> <code>abstractmethod</code> \u00b6 <p>Undo normalization.</p> <p>Parameters:</p> Name Type Description Default <code>normalized_data</code> <code>Tensor</code> <p>Normalized tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Original tensor.</p> Source code in <code>gridfm_graphkit/datasets/data_normalization.py</code> <pre><code>@abstractmethod\ndef inverse_transform(self, normalized_data: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Undo normalization.\n\n    Args:\n        normalized_data: Normalized tensor.\n\n    Returns:\n        Original tensor.\n    \"\"\"\n</code></pre> <code>transform(data)</code> <code>abstractmethod</code> \u00b6 <p>Normalize the input data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized tensor.</p> Source code in <code>gridfm_graphkit/datasets/data_normalization.py</code> <pre><code>@abstractmethod\ndef transform(self, data: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Normalize the input data.\n\n    Args:\n        data: Input tensor.\n\n    Returns:\n        Normalized tensor.\n    \"\"\"\n</code></pre>"},{"location":"datasets/data_normalization/#minmaxnormalizer","title":"<code>MinMaxNormalizer</code>","text":"<p>               Bases: <code>Normalizer</code></p> <p>Scales each feature to the [0, 1] range.</p> Source code in <code>gridfm_graphkit/datasets/data_normalization.py</code> <pre><code>class MinMaxNormalizer(Normalizer):\n    \"\"\"\n    Scales each feature to the [0, 1] range.\n    \"\"\"\n\n    def __init__(self):\n        self.min_val = None\n        self.max_val = None\n\n    def to(self, device):\n        self.min_val = self.min_val.to(device)\n        self.max_val = self.max_val.to(device)\n\n    def fit(self, data: torch.Tensor) -&gt; dict:\n        self.min_val, _ = data.min(axis=0)\n        self.max_val, _ = data.max(axis=0)\n\n        return {\"min_value\": self.min_val, \"max_value\": self.max_val}\n\n    def fit_from_dict(self, params: dict):\n        if self.min_val is None:\n            self.min_val = params.get(\"min_value\")\n        if self.max_val is None:\n            self.max_val = params.get(\"max_value\")\n\n    def transform(self, data: torch.Tensor) -&gt; torch.Tensor:\n        if self.min_val is None or self.max_val is None:\n            raise ValueError(\"fit must be called before transform.\")\n\n        diff = self.max_val - self.min_val\n        diff[diff == 0] = 1  # Avoid division by zero for features with zero range\n        return (data - self.min_val) / diff\n\n    def inverse_transform(self, normalized_data: torch.Tensor) -&gt; torch.Tensor:\n        if self.min_val is None or self.max_val is None:\n            raise ValueError(\"fit must be called before inverse_transform.\")\n\n        diff = self.max_val - self.min_val\n        diff[diff == 0] = 1\n        return (normalized_data * diff) + self.min_val\n</code></pre>"},{"location":"datasets/data_normalization/#standardizer","title":"<code>Standardizer</code>","text":"<p>               Bases: <code>Normalizer</code></p> <p>Standardizes each feature to zero mean and unit variance.</p> Source code in <code>gridfm_graphkit/datasets/data_normalization.py</code> <pre><code>class Standardizer(Normalizer):\n    \"\"\"\n    Standardizes each feature to zero mean and unit variance.\n    \"\"\"\n\n    def __init__(self):\n        self.mean = None\n        self.std = None\n\n    def to(self, device):\n        self.mean = self.mean.to(device)\n        self.std = self.std.to(device)\n\n    def fit(self, data: torch.Tensor) -&gt; dict:\n        self.mean = data.mean(axis=0)\n        self.std = data.std(axis=0)\n\n        return {\"mean_value\": self.mean, \"std_value\": self.std}\n\n    def fit_from_dict(self, params: dict):\n        if self.mean is None:\n            self.mean = params.get(\"mean_value\")\n        if self.std is None:\n            self.std = params.get(\"std_value\")\n\n    def transform(self, data: torch.Tensor) -&gt; torch.Tensor:\n        if self.mean is None or self.std is None:\n            raise ValueError(\"fit must be called before transform.\")\n\n        std = self.std.clone()\n        std[std == 0] = 1  # Avoid division by zero for features with zero std\n        return (data - self.mean) / std\n\n    def inverse_transform(self, normalized_data: torch.Tensor) -&gt; torch.Tensor:\n        if self.mean is None or self.std is None:\n            raise ValueError(\"fit must be called before inverse_transform.\")\n\n        std = self.std.clone()\n        std[std == 0] = 1\n        return (normalized_data * std) + self.mean\n</code></pre>"},{"location":"datasets/data_normalization/#basemvanormalizer","title":"<code>BaseMVANormalizer</code>","text":"<p>               Bases: <code>Normalizer</code></p> <p>In power systems, a suitable normalization strategy must preserve the physical properties of the system. A known method is the conversion to the per-unit (p.u.) system, which expresses electrical quantities such as voltage, current, power, and impedance as fractions of predefined base values. These base values are usually chosen based on system parameters, such as rated voltage. The per-unit conversion ensures that power system equations remain scale-invariant, preserving fundamental physical relationships.</p> Source code in <code>gridfm_graphkit/datasets/data_normalization.py</code> <pre><code>class BaseMVANormalizer(Normalizer):\n    \"\"\"\n    In power systems, a suitable normalization strategy must preserve the physical properties of\n    the system. A known method is the conversion to the per-unit (p.u.) system, which expresses\n    electrical quantities such as voltage, current, power, and impedance as fractions of predefined\n    base values. These base values are usually chosen based on system parameters, such as rated\n    voltage. The per-unit conversion ensures that power system equations remain scale-invariant,\n    preserving fundamental physical relationships.\n    \"\"\"\n\n    def __init__(self, node_data: bool, baseMVA_orig: float = 100.0):\n        \"\"\"\n        Args:\n            node_data: Whether data is node-level or edge-level (PD, QD, PG, QG, VA).\n            baseMVA_orig: Original baseMVA (e.g. from MATPOWER).\n        \"\"\"\n        self.node_data = node_data\n        self.baseMVA_orig = baseMVA_orig\n        self.baseMVA = None\n\n    def to(self, device):\n        pass\n\n    def fit(self, data: torch.Tensor, baseMVA: float = None) -&gt; dict:\n        if self.node_data:\n            self.baseMVA = data[:, [PD, QD, PG, QG]].max()\n        else:\n            self.baseMVA = baseMVA\n\n        return {\"baseMVA_orig\": self.baseMVA_orig, \"baseMVA\": self.baseMVA}\n\n    def fit_from_dict(self, params: dict):\n        if self.baseMVA is None:\n            self.baseMVA = params.get(\"baseMVA\")\n        if self.baseMVA_orig is None:\n            self.baseMVA_orig = params.get(\"baseMVA_orig\")\n\n    def transform(self, data: torch.Tensor) -&gt; torch.Tensor:\n        if self.baseMVA is None:\n            raise ValueError(\"BaseMVA is not specified\")\n\n        if self.baseMVA == 0:\n            raise ZeroDivisionError(\"BaseMVA is 0.\")\n\n        if self.node_data:\n            data[:, PD] = data[:, PD] / self.baseMVA\n            data[:, QD] = data[:, QD] / self.baseMVA\n            data[:, PG] = data[:, PG] / self.baseMVA\n            data[:, QG] = data[:, QG] / self.baseMVA\n            data[:, VA] = data[:, VA] * torch.pi / 180.0\n        else:\n            data = data * self.baseMVA_orig / self.baseMVA\n\n        return data\n\n    def inverse_transform(self, normalized_data: torch.Tensor) -&gt; torch.Tensor:\n        if self.baseMVA is None:\n            raise ValueError(\"fit must be called before inverse_transform.\")\n\n        if self.node_data:\n            normalized_data[:, PD] = normalized_data[:, PD] * self.baseMVA\n            normalized_data[:, QD] = normalized_data[:, QD] * self.baseMVA\n            normalized_data[:, PG] = normalized_data[:, PG] * self.baseMVA\n            normalized_data[:, QG] = normalized_data[:, QG] * self.baseMVA\n            normalized_data[:, VA] = normalized_data[:, VA] * 180.0 / torch.pi\n        else:\n            normalized_data = normalized_data * self.baseMVA / self.baseMVA_orig\n\n        return normalized_data\n</code></pre> <code>__init__(node_data, baseMVA_orig=100.0)</code> \u00b6 <p>Parameters:</p> Name Type Description Default <code>node_data</code> <code>bool</code> <p>Whether data is node-level or edge-level (PD, QD, PG, QG, VA).</p> required <code>baseMVA_orig</code> <code>float</code> <p>Original baseMVA (e.g. from MATPOWER).</p> <code>100.0</code> Source code in <code>gridfm_graphkit/datasets/data_normalization.py</code> <pre><code>def __init__(self, node_data: bool, baseMVA_orig: float = 100.0):\n    \"\"\"\n    Args:\n        node_data: Whether data is node-level or edge-level (PD, QD, PG, QG, VA).\n        baseMVA_orig: Original baseMVA (e.g. from MATPOWER).\n    \"\"\"\n    self.node_data = node_data\n    self.baseMVA_orig = baseMVA_orig\n    self.baseMVA = None\n</code></pre>"},{"location":"datasets/data_normalization/#identitynormalizer","title":"<code>IdentityNormalizer</code>","text":"<p>               Bases: <code>Normalizer</code></p> <p>No normalization: returns data unchanged.</p> Source code in <code>gridfm_graphkit/datasets/data_normalization.py</code> <pre><code>class IdentityNormalizer(Normalizer):\n    \"\"\"\n    No normalization: returns data unchanged.\n    \"\"\"\n\n    def fit(self, data: torch.Tensor) -&gt; dict:\n        return {}\n\n    def fit_from_dict(self, params: dict):\n        pass\n\n    def transform(self, data: torch.Tensor) -&gt; torch.Tensor:\n        return data\n\n    def inverse_transform(self, normalized_data: torch.Tensor) -&gt; torch.Tensor:\n        return normalized_data\n</code></pre>"},{"location":"datasets/data_normalization/#usage-workflow","title":"Usage Workflow","text":"<p>Example:</p> <pre><code>from gridfm_graphkit.datasets.data_normalization import MinMaxNormalizer\nimport torch\n\ndata = torch.randn(100, 5)  # Example tensor\n\nnormalizer = MinMaxNormalizer()\nparams = normalizer.fit(data)\nnormalized = normalizer.transform(data)\nrestored = normalizer.inverse_transform(normalized)\n</code></pre>"},{"location":"datasets/powergrid/","title":"Power Grid datasets","text":""},{"location":"datasets/powergrid/#griddatasetmem","title":"<code>GridDatasetMem</code>","text":"<p>               Bases: <code>InMemoryDataset</code></p> <p>A PyTorch Geometric <code>InMemoryDataset</code> for power grid data stored in tabular CSV format. This dataset class reads node and edge data from CSV files, applies normalization using user-specified <code>Normalizer</code> instances, and builds graph data objects with edge weights and positional encodings.</p> <ul> <li>Reads raw node and edge CSV files (<code>pf_node.csv</code>, <code>pf_edge.csv</code>).</li> <li>Applies the normalization method specified on both node and edge features</li> <li>Stores normalization statistics in the <code>processed</code> directory for reuse.</li> <li>Constructs <code>torch_geometric.data.Data</code> objects with edge weights and positional encodings (via random walk embeddings).</li> </ul> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory where the dataset is stored.</p> required <code>norm_method</code> <code>str</code> <p>Identifier for normalization method (e.g., \"minmax\", \"standard\").</p> required <code>node_normalizer</code> <code>Normalizer</code> <p>Normalizer used for node features.</p> required <code>edge_normalizer</code> <code>Normalizer</code> <p>Normalizer used for edge features.</p> required <code>pe_dim</code> <code>int</code> <p>Length of the random walk used for positional encoding.</p> required <code>mask_dim</code> <code>int</code> <p>Number of features per-node that could be masked. Usually Pd, Qd, Pg, Qg, Vm, Va</p> <code>6</code> <code>transform</code> <code>callable</code> <p>Transformation applied at runtime.</p> <code>None</code> <code>pre_transform</code> <code>callable</code> <p>Transformation applied before saving to disk.</p> <code>None</code> <code>pre_filter</code> <code>callable</code> <p>Filter to determine which graphs to keep.</p> <code>None</code> Source code in <code>gridfm_graphkit/datasets/powergrid.py</code> <pre><code>class GridDatasetMem(InMemoryDataset):\n    \"\"\"\n    A PyTorch Geometric `InMemoryDataset` for power grid data stored in tabular CSV format.\n    This dataset class reads node and edge data from CSV files, applies normalization using\n    user-specified `Normalizer` instances, and builds graph data objects with edge weights and\n    positional encodings.\n\n    - Reads raw node and edge CSV files (`pf_node.csv`, `pf_edge.csv`).\n    - Applies the normalization method specified on both node and edge features\n    - Stores normalization statistics in the `processed` directory for reuse.\n    - Constructs `torch_geometric.data.Data` objects with edge weights and positional encodings (via random walk embeddings).\n\n    Args:\n        root (str): Root directory where the dataset is stored.\n        norm_method (str): Identifier for normalization method (e.g., \"minmax\", \"standard\").\n        node_normalizer (Normalizer): Normalizer used for node features.\n        edge_normalizer (Normalizer): Normalizer used for edge features.\n        pe_dim (int): Length of the random walk used for positional encoding.\n        mask_dim (int, optional): Number of features per-node that could be masked. Usually Pd, Qd, Pg, Qg, Vm, Va\n        transform (callable, optional): Transformation applied at runtime.\n        pre_transform (callable, optional): Transformation applied before saving to disk.\n        pre_filter (callable, optional): Filter to determine which graphs to keep.\n    \"\"\"\n\n    def __init__(\n        self,\n        root: str,\n        norm_method: str,\n        node_normalizer: Normalizer,\n        edge_normalizer: Normalizer,\n        pe_dim: int,\n        mask_dim: int = 6,\n        transform: Optional[Callable] = None,\n        pre_transform: Optional[Callable] = None,\n        pre_filter: Optional[Callable] = None,\n    ):\n        self.norm_method = norm_method\n        self.node_normalizer = node_normalizer\n        self.edge_normalizer = edge_normalizer\n        self.pe_dim = pe_dim\n        self.mask_dim = mask_dim\n        self.original_transform = None\n\n        super().__init__(root, transform, pre_transform, pre_filter)\n\n        node_stats_path = osp.join(\n            self.processed_dir,\n            f\"node_stats_{self.norm_method}.pt\",\n        )\n        edge_stats_path = osp.join(\n            self.processed_dir,\n            f\"edge_stats_{self.norm_method}.pt\",\n        )\n        if osp.exists(node_stats_path) and osp.exists(edge_stats_path):\n            self.node_stats = torch.load(node_stats_path, weights_only=False)\n            self.edge_stats = torch.load(edge_stats_path, weights_only=False)\n            self.node_normalizer.fit_from_dict(self.node_stats)\n            self.edge_normalizer.fit_from_dict(self.edge_stats)\n        self.load(self.processed_paths[0])\n\n    @property\n    def raw_file_names(self):\n        # No raw files needed for random graphs\n        return [\"pf_node.csv\", \"pf_edge.csv\"]\n\n    @property\n    def processed_file_names(self):\n        return [f\"data_full_{self.norm_method}.pt\"]\n\n    def download(self):\n        pass\n\n    def process(self):\n        node_df = pd.read_csv(osp.join(self.raw_dir, \"pf_node.csv\"))\n        edge_df = pd.read_csv(osp.join(self.raw_dir, \"pf_edge.csv\"))\n\n        # Check the unique scenarios available\n        scenarios = node_df[\"scenario\"].unique()\n        # Ensure node and edge data match\n        if not (scenarios == edge_df[\"scenario\"].unique()).all():\n            raise ValueError(\"Mismatch between node and edge scenario values.\")\n\n        # normalize node attributes\n        cols_to_normalize = [\"Pd\", \"Qd\", \"Pg\", \"Qg\", \"Vm\", \"Va\"]\n        to_normalize = torch.tensor(\n            node_df[cols_to_normalize].values,\n            dtype=torch.float,\n        )\n        self.node_stats = self.node_normalizer.fit(to_normalize)\n        node_df[cols_to_normalize] = self.node_normalizer.transform(\n            to_normalize,\n        ).numpy()\n\n        # normalize edge attributes\n        cols_to_normalize = [\"G\", \"B\"]\n        to_normalize = torch.tensor(\n            edge_df[cols_to_normalize].values,\n            dtype=torch.float,\n        )\n        if isinstance(self.node_normalizer, BaseMVANormalizer):\n            self.edge_stats = self.edge_normalizer.fit(\n                to_normalize,\n                self.node_normalizer.baseMVA,\n            )\n        else:\n            self.edge_stats = self.edge_normalizer.fit(to_normalize)\n        edge_df[cols_to_normalize] = self.edge_normalizer.transform(\n            to_normalize,\n        ).numpy()\n\n        # save stats\n        node_stats_path = osp.join(\n            self.processed_dir,\n            f\"node_stats_{self.norm_method}.pt\",\n        )\n        edge_stats_path = osp.join(\n            self.processed_dir,\n            f\"edge_stats_{self.norm_method}.pt\",\n        )\n        torch.save(self.node_stats, node_stats_path)\n        torch.save(self.edge_stats, edge_stats_path)\n\n        # Create groupby objects for scenarios\n        node_groups = node_df.groupby(\"scenario\")\n        edge_groups = edge_df.groupby(\"scenario\")\n\n        data_list = []\n        for scenario_idx in tqdm(scenarios):\n            # NODE DATA\n            node_data = node_groups.get_group(scenario_idx)\n            x = torch.tensor(\n                node_data[\n                    [\"Pd\", \"Qd\", \"Pg\", \"Qg\", \"Vm\", \"Va\", \"PQ\", \"PV\", \"REF\"]\n                ].values,\n                dtype=torch.float,\n            )\n            y = x[:, : self.mask_dim]\n\n            # EDGE DATA\n            edge_data = edge_groups.get_group(scenario_idx)\n            edge_attr = torch.tensor(edge_data[[\"G\", \"B\"]].values, dtype=torch.float)\n            edge_index = torch.tensor(\n                edge_data[[\"index1\", \"index2\"]].values.T,\n                dtype=torch.long,\n            )\n\n            # Create the Data object\n            graph_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n            pe_pre_transform = AddEdgeWeights()\n            graph_data = pe_pre_transform(graph_data)\n            pe_transform = AddNormalizedRandomWalkPE(\n                walk_length=self.pe_dim,\n                attr_name=\"pe\",\n            )\n            graph_data = pe_transform(graph_data)\n            data_list.append(graph_data)\n\n        self.save(data_list, self.processed_paths[0])\n\n    def change_transform(self, new_transform):\n        \"\"\"\n        Temporarily switch to a new transform function, used when evaluating different tasks.\n\n        Args:\n            new_transform (Callable): The new transform to use.\n        \"\"\"\n        self.original_transform = self.transform\n        self.transform = new_transform\n\n    def reset_transform(self):\n        \"\"\"\n        Reverts the transform to the original one set during initialization, usually called after the evaluation step.\n        \"\"\"\n        if self.original_transform is None:\n            raise ValueError(\n                \"The original transform is None or the function change_transform needs to be called before\",\n            )\n        self.transform = self.original_transform\n</code></pre> <code>change_transform(new_transform)</code> \u00b6 <p>Temporarily switch to a new transform function, used when evaluating different tasks.</p> <p>Parameters:</p> Name Type Description Default <code>new_transform</code> <code>Callable</code> <p>The new transform to use.</p> required Source code in <code>gridfm_graphkit/datasets/powergrid.py</code> <pre><code>def change_transform(self, new_transform):\n    \"\"\"\n    Temporarily switch to a new transform function, used when evaluating different tasks.\n\n    Args:\n        new_transform (Callable): The new transform to use.\n    \"\"\"\n    self.original_transform = self.transform\n    self.transform = new_transform\n</code></pre> <code>reset_transform()</code> \u00b6 <p>Reverts the transform to the original one set during initialization, usually called after the evaluation step.</p> Source code in <code>gridfm_graphkit/datasets/powergrid.py</code> <pre><code>def reset_transform(self):\n    \"\"\"\n    Reverts the transform to the original one set during initialization, usually called after the evaluation step.\n    \"\"\"\n    if self.original_transform is None:\n        raise ValueError(\n            \"The original transform is None or the function change_transform needs to be called before\",\n        )\n    self.transform = self.original_transform\n</code></pre>"},{"location":"datasets/powergrid/#usage-example","title":"Usage Example","text":"<pre><code>from gridfm_graphkit.datasets.data_normalization import IdentityNormalizer\nfrom gridfm_graphkit.datasets.powergrid import GridDatasetMem\n\ndataset = GridDatasetMem(\n    root=\"./data\",\n    norm_method=\"identity\",\n    node_normalizer=IdentityNormalizer(),\n    edge_normalizer=IdentityNormalizer(),\n    pe_dim=10,\n    mask_dim=6,\n)\n</code></pre>"},{"location":"datasets/transforms/","title":"Transforms","text":"<p>Each transformation class inherits from <code>BaseTransform</code> provided by PyTorch Geometric.</p>"},{"location":"datasets/transforms/#addnormalizedrandomwalkpe","title":"<code>AddNormalizedRandomWalkPE</code>","text":"<p>               Bases: <code>BaseTransform</code></p> <p>Adds the random walk positional encoding from the Graph Neural Networks with Learnable Structural and Positional Representations paper to the given graph. This is an adaptation from the original Pytorch Geometric implementation.</p> <p>Parameters:</p> Name Type Description Default <code>walk_length</code> <code>int</code> <p>The number of random walk steps.</p> required <code>attr_name</code> <code>str</code> <p>The attribute name of the data object to add positional encodings to. If set to :obj:<code>None</code>, will be concatenated to :obj:<code>data.x</code>. (default: :obj:<code>\"random_walk_pe\"</code>)</p> <code>'random_walk_pe'</code> Source code in <code>gridfm_graphkit/datasets/transforms.py</code> <pre><code>class AddNormalizedRandomWalkPE(BaseTransform):\n    r\"\"\"Adds the random walk positional encoding from the\n    [Graph Neural Networks with Learnable Structural and Positional Representations](https://arxiv.org/abs/2110.07875)\n    paper to the given graph. This is an adaptation from the original Pytorch Geometric implementation.\n\n    Args:\n        walk_length (int): The number of random walk steps.\n        attr_name (str, optional): The attribute name of the data object to add\n            positional encodings to. If set to :obj:`None`, will be\n            concatenated to :obj:`data.x`.\n            (default: :obj:`\"random_walk_pe\"`)\n    \"\"\"\n\n    def __init__(\n        self,\n        walk_length: int,\n        attr_name: Optional[str] = \"random_walk_pe\",\n    ) -&gt; None:\n        self.walk_length = walk_length\n        self.attr_name = attr_name\n\n    def forward(self, data: Data) -&gt; Data:\n        if data.edge_index is None:\n            raise ValueError(\"Expected data.edge_index to be not None\")\n        row, col = data.edge_index\n        N = data.num_nodes\n        if N is None:\n            raise ValueError(\"Expected data.num_nodes to be not None\")\n\n        if N &lt;= 2_000:  # Dense code path for faster computation:\n            adj = torch.zeros((N, N), device=row.device)\n            adj[row, col] = data.edge_weight\n            loop_index = torch.arange(N, device=row.device)\n        elif torch_geometric.typing.WITH_WINDOWS:\n            adj = to_torch_coo_tensor(\n                data.edge_index,\n                data.edge_weight,\n                size=data.size(),\n            )\n        else:\n            adj = to_torch_csr_tensor(\n                data.edge_index,\n                data.edge_weight,\n                size=data.size(),\n            )\n\n        row_sums = adj.sum(dim=1, keepdim=True)  # Sum along rows\n        row_sums = row_sums.clamp(min=1e-8)  # Prevent division by zero\n\n        adj = adj / row_sums  # Normalize each row to sum to 1\n\n        def get_pe(out: Tensor) -&gt; Tensor:\n            if is_torch_sparse_tensor(out):\n                return get_self_loop_attr(*to_edge_index(out), num_nodes=N)\n            return out[loop_index, loop_index]\n\n        out = adj\n        pe_list = [get_pe(out)]\n        for _ in range(self.walk_length - 1):\n            out = out @ adj\n            pe_list.append(get_pe(out))\n\n        pe = torch.stack(pe_list, dim=-1)\n        data[self.attr_name] = pe\n\n        return data\n</code></pre>"},{"location":"datasets/transforms/#addedgeweights","title":"<code>AddEdgeWeights</code>","text":"<p>               Bases: <code>BaseTransform</code></p> <p>Computes and adds edge weight as the magnitude of complex admittance.</p> <p>The magnitude is computed from the G and B components in <code>data.edge_attr</code> and stored in <code>data.edge_weight</code>.</p> Source code in <code>gridfm_graphkit/datasets/transforms.py</code> <pre><code>class AddEdgeWeights(BaseTransform):\n    \"\"\"\n    Computes and adds edge weight as the magnitude of complex admittance.\n\n    The magnitude is computed from the G and B components in `data.edge_attr` and stored in `data.edge_weight`.\n    \"\"\"\n\n    def forward(self, data):\n        if not hasattr(data, \"edge_attr\"):\n            raise AttributeError(\"Data must have 'edge_attr'.\")\n\n        # Extract real and imaginary parts of admittance\n        real = data.edge_attr[:, G]\n        imag = data.edge_attr[:, B]\n\n        # Compute the magnitude of the complex admittance\n        edge_weight = torch.sqrt(real**2 + imag**2)\n\n        # Add the computed edge weights to the data object\n        data.edge_weight = edge_weight\n\n        return data\n</code></pre>"},{"location":"datasets/transforms/#addidentitymask","title":"<code>AddIdentityMask</code>","text":"<p>               Bases: <code>BaseTransform</code></p> <p>Creates an identity mask, and adds it as a <code>mask</code> attribute.</p> <p>The mask is generated such that every entry is False, so no masking is actually applied</p> Source code in <code>gridfm_graphkit/datasets/transforms.py</code> <pre><code>class AddIdentityMask(BaseTransform):\n    \"\"\"Creates an identity mask, and adds it as a `mask` attribute.\n\n    The mask is generated such that every entry is False, so no masking is actually applied\n    \"\"\"\n\n    def forward(self, data):\n        if not hasattr(data, \"y\"):\n            raise AttributeError(\"Data must have ground truth 'y'.\")\n\n        # Generate an identity mask\n        mask = torch.zeros_like(data.y, dtype=torch.bool)\n\n        # Add the mask to the data object\n        data.mask = mask\n\n        return data\n</code></pre>"},{"location":"datasets/transforms/#addrandommask","title":"<code>AddRandomMask</code>","text":"<p>               Bases: <code>BaseTransform</code></p> <p>Creates a random mask, and adds it as a <code>mask</code> attribute.</p> <p>The mask is generated such that each entry is <code>True</code> with probability <code>mask_ratio</code> and <code>False</code> otherwise.</p> Source code in <code>gridfm_graphkit/datasets/transforms.py</code> <pre><code>class AddRandomMask(BaseTransform):\n    \"\"\"Creates a random mask, and adds it as a `mask` attribute.\n\n    The mask is generated such that each entry is `True` with probability\n    `mask_ratio` and `False` otherwise.\n    \"\"\"\n\n    def __init__(self, mask_dim, mask_ratio):\n        super().__init__()\n        self.mask_dim = mask_dim\n        self.mask_ratio = mask_ratio\n\n    def forward(self, data):\n        if not hasattr(data, \"x\"):\n            raise AttributeError(\"Data must have node features 'x'.\")\n\n        # Generate a random mask\n        mask = torch.rand(data.x.size(0), self.mask_dim) &lt; self.mask_ratio\n\n        # Add the mask to the data object\n        data.mask = mask\n\n        return data\n</code></pre>"},{"location":"datasets/transforms/#addpfmask","title":"<code>AddPFMask</code>","text":"<p>               Bases: <code>BaseTransform</code></p> <p>Creates a mask according to the power flow problem and assigns it as a <code>mask</code> attribute.</p> Source code in <code>gridfm_graphkit/datasets/transforms.py</code> <pre><code>class AddPFMask(BaseTransform):\n    \"\"\"Creates a mask according to the power flow problem and assigns it as a `mask` attribute.\"\"\"\n\n    def forward(self, data):\n        # Ensure the data object has the required attributes\n        if not hasattr(data, \"y\"):\n            raise AttributeError(\"Data must have ground truth 'y'.\")\n\n        if not hasattr(data, \"x\"):\n            raise AttributeError(\"Data must have node features 'x'.\")\n\n        # Generate masks for each type of node\n        mask_PQ = data.x[:, PQ] == 1  # PQ buses\n        mask_PV = data.x[:, PV] == 1  # PV buses\n        mask_REF = data.x[:, REF] == 1  # Reference buses\n\n        # Initialize the mask tensor with False values\n        mask = torch.zeros_like(data.y, dtype=torch.bool)\n\n        mask[mask_PQ, VM] = True  # Mask Vm for PQ buses\n        mask[mask_PQ, VA] = True  # Mask Va for PQ buses\n\n        mask[mask_PV, QG] = True  # Mask Qg for PV buses\n        mask[mask_PV, VA] = True  # Mask Va for PV buses\n\n        mask[mask_REF, PG] = True  # Mask Pg for REF buses\n        mask[mask_REF, QG] = True  # Mask Qg for REF buses\n\n        # Attach the mask to the data object\n        data.mask = mask\n\n        return data\n</code></pre>"},{"location":"datasets/transforms/#addopfmask","title":"<code>AddOPFMask</code>","text":"<p>               Bases: <code>BaseTransform</code></p> <p>Creates a mask according to the optimal power flow problem and assigns it as a <code>mask</code> attribute.</p> Source code in <code>gridfm_graphkit/datasets/transforms.py</code> <pre><code>class AddOPFMask(BaseTransform):\n    \"\"\"Creates a mask according to the optimal power flow problem and assigns it as a `mask` attribute.\"\"\"\n\n    def forward(self, data):\n        # Ensure the data object has the required attributes\n        if not hasattr(data, \"y\"):\n            raise AttributeError(\"Data must have ground truth 'y'.\")\n\n        if not hasattr(data, \"x\"):\n            raise AttributeError(\"Data must have node features 'x'.\")\n\n        # Generate masks for each type of node\n        mask_PQ = data.x[:, PQ] == 1  # PQ buses\n        mask_PV = data.x[:, PV] == 1  # PV buses\n        mask_REF = data.x[:, REF] == 1  # Reference buses\n\n        # Initialize the mask tensor with False values\n        mask = torch.zeros_like(data.y, dtype=torch.bool)\n\n        mask[mask_PQ, VM] = True  # Mask Vm for PQ\n        mask[mask_PQ, VA] = True  # Mask Va for PQ\n\n        mask[mask_PV, PG] = True  # Mask Pg for PV\n        mask[mask_PV, QG] = True  # Mask Qg for PV\n        mask[mask_PV, VM] = True  # Mask Vm for PV\n        mask[mask_PV, VA] = True  # Mask Va for PV\n\n        mask[mask_REF, PG] = True  # Mask Pg for REF\n        mask[mask_REF, QG] = True  # Mask Qg for REF\n        mask[mask_REF, VM] = True  # Mask Vm for REF\n        mask[mask_REF, VA] = True  # Mask Va for REF\n\n        # Attach the mask to the data object\n        data.mask = mask\n\n        return data\n</code></pre>"},{"location":"examples/fine_tuning/","title":"Fine-tuning an existing GridFM","text":"<p>Here we demonstrate how to leverage a previously pre-trained model to perform fine-tuning on downstream tasks. Specifically, we focus on the Power Flow (PF) problem, a fundamental task in power systems that involves computing the steady-state voltages and power injections in the grid.</p> <p>The workflow consists of the following steps:</p> <ul> <li> <p>Similar to pre-training, the first step is to normalize the data and convert the power grid into a PyTorch Geometric graph representation.</p> </li> <li> <p>A DataLoader then loads the data for fine-tuning.</p> </li> <li> <p>In the PF use case, which closely aligns with the pre-training setup, we adjust the masking strategy to match the PF problem, i.e. no longer using random masking. For other use cases, it may be necessary to modify the decoder or add additional heads or decoder layers to the pre-trained autoencoder.</p> </li> <li> <p>The model is then trained to reconstruct the PF grid state. The loss function consists of a physics-informed loss based on node-wise power balance equations (ensuring power injected equals power consumed or absorbed).</p> </li> </ul> \\[ \\mathcal{L}_{\\text{PBE}} = \\frac{1}{N} \\sum_{i=1}^N \\left| (P_{G,i} - P_{D,i}) + j(Q_{G,i} - Q_{D,i}) - S_{\\text{injection}, i} \\right| \\] <ul> <li>Finally, we visualize fine-tuning performance.</li> </ul> <pre><code>from gridfm_graphkit.datasets.powergrid import GridDatasetMem\nfrom gridfm_graphkit.datasets.data_normalization import BaseMVANormalizer\nfrom gridfm_graphkit.io.param_handler import NestedNamespace, get_transform, load_model, get_loss_function\nfrom gridfm_graphkit.training.trainer import Trainer\nfrom gridfm_graphkit.datasets.utils import split_dataset\nfrom gridfm_graphkit.datasets.transforms import AddPFMask\nfrom gridfm_graphkit.training.callbacks import EarlyStopper\nfrom gridfm_graphkit.training.plugins import MetricsTrackerPlugin\nfrom gridfm_graphkit.utils.loss import PBELoss\n\n# Standard Libraries\nimport torch\nfrom torch_geometric.loader import DataLoader\nfrom torch.utils.data import Subset\nimport matplotlib.pyplot as plt\nimport os\nimport yaml\nimport warnings\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n</code></pre>"},{"location":"examples/fine_tuning/#load-the-training-data-and-create-the-dataset","title":"Load the training data and create the dataset","text":"<pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n</code></pre> <pre><code># Select from which grid case file the simulated AC powerflow data should be used\ndata_dir = \"../data/case30_ieee\"\n\nnode_normalizer, edge_normalizer = BaseMVANormalizer(node_data=True), BaseMVANormalizer(node_data=False)\n\ndataset = GridDatasetMem(\n    root=data_dir,\n    norm_method=\"baseMVAnorm\",\n    node_normalizer=node_normalizer,\n    edge_normalizer=edge_normalizer,\n    pe_dim=20,           # Dimension of positional encoding\n    transform=AddPFMask()\n)\n</code></pre>"},{"location":"examples/fine_tuning/#split-the-dataset-for-training-and-validation","title":"Split the dataset for training and validation","text":"<pre><code>node_normalizer.to(device)\nedge_normalizer.to(device)\n\ntrain_dataset, val_dataset, _ = split_dataset(\n    dataset, data_dir, val_ratio=0.1, test_ratio=0.1\n)\n</code></pre>"},{"location":"examples/fine_tuning/#create-pytorch-dataloaders-for-training-validation-and-testing","title":"Create Pytorch dataloaders for training, validation and testing","text":"<pre><code># Create DataLoaders with batches. The data-Loaders also take care of the masking for the powerflow problem formulation, the masking strategy in the configuration yaml needs to be set to \"pf\".\ntrain_loader = DataLoader(\n    train_dataset, batch_size=32, shuffle=True\n)\nval_loader = DataLoader(\n    val_dataset, batch_size=32, shuffle=False\n)\n</code></pre>"},{"location":"examples/fine_tuning/#load-the-model","title":"Load the model","text":"<pre><code>model = torch.load(\"../models/GridFM_v0_2_3.pth\", weights_only=False, map_location=device).to(device)\n\n# Select optimizer and learning rate scheduler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\noptimizer = torch.optim.Adam(\n    model.parameters(),\n    lr=0.0001,\n)\n# Adjust learning rate while training\nscheduler = ReduceLROnPlateau(optimizer)\n\n# This block only for compatibility with original code - does not do anything here\nbest_model_path = os.path.join(\"best_checkpoint.pth\")\nearly_stopper = EarlyStopper(\n    best_model_path, -1, 0\n)\n</code></pre>"},{"location":"examples/fine_tuning/#fine-tune-the-model","title":"Fine-tune the model","text":"<pre><code>loss_fn = PBELoss()\n# Plugin logs validation losses and saves to file for later use\nlog_val_loss_plugin = MetricsTrackerPlugin()\n# Setup Trainer Instance\ntrainer = Trainer(\n    model=model,\n    optimizer=optimizer,\n    device=device,\n    loss_fn=loss_fn,\n    early_stopper=early_stopper,\n    train_dataloader=train_loader,\n    val_dataloader=val_loader,\n    lr_scheduler=scheduler,\n    plugins=[log_val_loss_plugin],\n)\ntrainer.train(epochs=15)\n</code></pre>"},{"location":"examples/visualization/","title":"Visualizing predictions of GridFM","text":"<pre><code>from gridfm_graphkit.datasets.powergrid import GridDatasetMem\nfrom gridfm_graphkit.datasets.data_normalization import BaseMVANormalizer\nfrom gridfm_graphkit.utils.visualization import visualize_error, visualize_quantity_heatmap\nfrom gridfm_graphkit.datasets.globals import PD, QD, PG, QG, VM, VA\nfrom gridfm_graphkit.datasets.transforms import AddRandomMask\n\n# Standard open-source libraries\nimport torch\nfrom torch_geometric.loader import DataLoader\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport time\nimport random\n\ntorch.manual_seed(0)\nrandom.seed(0)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n</code></pre>"},{"location":"examples/visualization/#load-and-normalize-the-power-grid-dataset-for-grid-case30-from-ieee","title":"Load and normalize the power grid dataset for grid case30 from IEEE","text":"<pre><code># This network was chosen for visualization purposes (networks up to 300 buses were tested)\n# The number of load scenarios is 1024\nnetwork = \"../data/case30_ieee\"\nnode_normalizer, edge_normalizer = (\n    BaseMVANormalizer(node_data=True),\n    BaseMVANormalizer(node_data=False),\n)\ndataset = GridDatasetMem(\n    root=network,\n    norm_method=\"baseMVAnorm\",\n    node_normalizer=node_normalizer,\n    edge_normalizer=edge_normalizer,\n    pe_dim=20,\n    transform=AddRandomMask(mask_dim=6, mask_ratio=0.5),\n)\n</code></pre>"},{"location":"examples/visualization/#create-a-pytorch-dataloader","title":"Create a Pytorch dataloader","text":"<pre><code># The scenarios are grouped in batches\nloader = DataLoader(dataset, batch_size=32)\n</code></pre>"},{"location":"examples/visualization/#load-gridfm-v02","title":"Load gridFM-v0.2","text":"<pre><code>model = torch.load(\"../models/GridFM_v0_2_3.pth\", weights_only=False, map_location=device).to(device)\n</code></pre>"},{"location":"examples/visualization/#state-reconstruction-of-1024-scenarios-6-features","title":"State reconstruction of 1024 scenarios (6 features)","text":"<pre><code>model.eval()\nwith torch.no_grad():\n    for batch in tqdm(loader):\n        batch = batch.to(device)\n\n        # Apply random masking\n        mask_value_expanded = model.mask_value.expand(batch.x.shape[0], -1)\n        batch.x[:, : batch.mask.shape[1]][batch.mask] = mask_value_expanded[batch.mask]\n\n        # Perform inference\n        output = model(\n            batch.x, batch.pe, batch.edge_index, batch.edge_attr, batch.batch\n        )\n</code></pre>"},{"location":"examples/visualization/#visualize-nodal-active-power-residuals-for-one-load-scenario","title":"Visualize nodal active power residuals for one load scenario","text":"<pre><code># select one random sample from the dataset\ndata_point = dataset[random.randint(0, len(dataset) - 1)]\n\nvisualize_error(data_point, model, baseMVA=node_normalizer.baseMVA, device=device)\n</code></pre>"},{"location":"examples/visualization/#visualize-the-state-reconstruction-capability-of-gridfm-v02-for-each-feature","title":"Visualize the state reconstruction capability of gridFM-v0.2 for each feature:","text":"<ul> <li>Active Power Demand (MW)</li> <li>Reactive Power Demand (MVar)</li> <li>Active Power Generated (MW)</li> <li>Reactive Power Generated (MVar)</li> <li>Voltage Magnitude (p.u.)</li> <li>Voltage Angle (degrees)</li> </ul> <pre><code># Active power demand reconstruction\nvisualize_quantity_heatmap(\n    data_point,\n    model,\n    PD,\n    \"Active Power Demand\",\n    \"MW\",\n    node_normalizer,\n    plt.cm.viridis,\n    device=device,\n)\n</code></pre> <pre><code># Reactive power demand reconstruction\nvisualize_quantity_heatmap(\n    data_point,\n    model,\n    QD,\n    \"Reactive Power Demand\",\n    \"MVar\",\n    node_normalizer,\n    plt.cm.plasma,\n    device=device,\n)\n</code></pre> <pre><code># Active power generated reconstruction\nvisualize_quantity_heatmap(\n    data_point,\n    model,\n    PG,\n    \"Active Power Generated\",\n    \"MW\",\n    node_normalizer,\n    plt.cm.viridis,\n    device=device,\n)\n</code></pre> <pre><code># Reactive power generated reconstruction\nvisualize_quantity_heatmap(\n    data_point,\n    model,\n    QG,\n    \"Reactive Power Generated\",\n    \"MVar\",\n    node_normalizer,\n    plt.cm.plasma,\n    device=device,\n)\n</code></pre> <pre><code># Voltage magnitude reconstruction\nvisualize_quantity_heatmap(\n    data_point,\n    model,\n    VM,\n    \"Voltage Magnitude\",\n    \"p.u.\",\n    node_normalizer,\n    plt.cm.magma,\n    device=device,\n)\n</code></pre> <pre><code># Voltage angle reconstruction\nvisualize_quantity_heatmap(\n    data_point,\n    model,\n    VA,\n    \"Voltage Angle\",\n    \"degrees\",\n    node_normalizer,\n    plt.cm.inferno,\n    device=device,\n)\n</code></pre>"},{"location":"install/installation/","title":"Installation","text":"<p>You can install <code>gridfm-graphkit</code> directly from PyPI:</p> <pre><code>pip install gridfm-graphkit\n</code></pre>"},{"location":"install/installation/#development-setup","title":"Development Setup","text":"<p>To contribute or develop locally, clone the repository and install in editable mode:</p> <pre><code>git clone git@github.com:gridfm/gridfm-graphkit.git\ncd gridfm-graphkit\npython -m venv venv\nsource venv/bin/activate\npip install -e .\n</code></pre> <p>For documentation generation and unit testing, install with the optional <code>dev</code> and <code>test</code> extras:</p> <pre><code>pip install -e .[dev,test]\n</code></pre>"},{"location":"models/models/","title":"Models","text":""},{"location":"models/models/#gpstransformer","title":"<code>GPSTransformer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A GPS (Graph Transformer) model based on GPSConv and GINEConv layers from Pytorch Geometric.</p> <p>This model encodes node features and positional encodings separately, then applies multiple graph convolution layers with batch normalization, and finally decodes to the output dimension.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimension of input node features.</p> required <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size for all layers.</p> required <code>output_dim</code> <code>int</code> <p>Dimension of the output node features.</p> required <code>edge_dim</code> <code>int</code> <p>Dimension of edge features.</p> required <code>pe_dim</code> <code>int</code> <p>Dimension of the positional encoding. Must be less than hidden_dim.</p> required <code>num_layers</code> <code>int</code> <p>Number of GPSConv layers.</p> required <code>heads</code> <code>int</code> <p>Number of attention heads in GPSConv.</p> <code>1</code> <code>dropout</code> <code>float</code> <p>Dropout rate in GPSConv.</p> <code>0.0</code> <code>mask_dim</code> <code>int</code> <p>Dimension of the mask vector.</p> <code>6</code> <code>mask_value</code> <code>float</code> <p>Initial value for learnable mask parameters.</p> <code>-1.0</code> <code>learn_mask</code> <code>bool</code> <p>Whether to learn mask values as parameters.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>pe_dim</code> is not less than <code>hidden_dim</code>.</p> Source code in <code>gridfm_graphkit/models/gps_transformer.py</code> <pre><code>class GPSTransformer(nn.Module):\n    \"\"\"\n    A GPS (Graph Transformer) model based on [GPSConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GPSConv.html) and [GINEConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GINEConv.html) layers from Pytorch Geometric.\n\n    This model encodes node features and positional encodings separately,\n    then applies multiple graph convolution layers with batch normalization,\n    and finally decodes to the output dimension.\n\n    Args:\n        input_dim (int): Dimension of input node features.\n        hidden_dim (int): Hidden dimension size for all layers.\n        output_dim (int): Dimension of the output node features.\n        edge_dim (int): Dimension of edge features.\n        pe_dim (int): Dimension of the positional encoding.\n            Must be less than hidden_dim.\n        num_layers (int): Number of GPSConv layers.\n        heads (int, optional): Number of attention heads in GPSConv.\n        dropout (float, optional): Dropout rate in GPSConv.\n        mask_dim (int, optional): Dimension of the mask vector.\n        mask_value (float, optional): Initial value for learnable mask parameters.\n        learn_mask (bool, optional): Whether to learn mask values as parameters.\n\n    Raises:\n        ValueError: If `pe_dim` is not less than `hidden_dim`.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int,\n        output_dim: int,\n        edge_dim: int,\n        pe_dim: int,\n        num_layers: int,\n        heads: int = 1,\n        dropout: float = 0.0,\n        mask_dim: int = 6,\n        mask_value: float = -1.0,\n        learn_mask: bool = True,\n    ):\n        super(GPSTransformer, self).__init__()\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.edge_dim = edge_dim\n        self.pe_dim = pe_dim\n        self.heads = heads\n        self.dropout = dropout\n        self.mask_dim = mask_dim\n        self.mask_value = mask_value\n        self.learn_mask = learn_mask\n\n        if not pe_dim &lt; hidden_dim:\n            raise ValueError(\n                \"positional encoding dimension must be smaller than model hidden dimension\",\n            )\n\n        self.layers = nn.ModuleList()\n\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, self.hidden_dim - self.pe_dim),\n            nn.LeakyReLU(),\n        )\n        self.input_norm = nn.BatchNorm1d(self.hidden_dim - self.pe_dim)\n        self.pe_norm = nn.BatchNorm1d(self.pe_dim)\n\n        for _ in range(self.num_layers):\n            mlp = nn.Sequential(\n                nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim),\n                nn.LeakyReLU(),\n            )\n            self.layers.append(\n                nn.ModuleDict(\n                    {\n                        \"conv\": GPSConv(\n                            channels=self.hidden_dim,\n                            conv=GINEConv(nn=mlp, edge_dim=self.edge_dim),\n                            heads=self.heads,\n                            dropout=self.dropout,\n                        ),\n                        \"norm\": nn.BatchNorm1d(\n                            self.hidden_dim,\n                        ),  # BatchNorm after each graph layer\n                    },\n                ),\n            )\n\n        self.pre_decoder_norm = nn.BatchNorm1d(self.hidden_dim)\n        # Fully connected (MLP) layers after the GAT layers\n        self.decoder = nn.Sequential(\n            nn.Linear(self.hidden_dim, self.hidden_dim),\n            nn.LeakyReLU(),\n            nn.Linear(self.hidden_dim, output_dim),\n        )\n\n        if learn_mask:\n            self.mask_value = nn.Parameter(\n                torch.randn(mask_dim) + mask_value,\n                requires_grad=True,\n            )\n        else:\n            self.mask_value = nn.Parameter(\n                torch.zeros(mask_dim) + mask_value,\n                requires_grad=False,\n            )\n\n    def forward(self, x, pe, edge_index, edge_attr, batch):\n        \"\"\"\n        Forward pass for the GPSTransformer.\n\n        Args:\n            x (Tensor): Input node features of shape [num_nodes, input_dim].\n            pe (Tensor): Positional encoding of shape [num_nodes, pe_dim].\n            edge_index (Tensor): Edge indices for graph convolution.\n            edge_attr (Tensor): Edge feature tensor.\n            batch (Tensor): Batch vector assigning nodes to graphs.\n\n        Returns:\n            output (Tensor): Output node features of shape [num_nodes, output_dim].\n        \"\"\"\n        x_pe = self.pe_norm(pe)\n\n        x = self.encoder(x)\n        x = self.input_norm(x)\n\n        x = torch.cat((x, x_pe), 1)\n        for layer in self.layers:\n            x = layer[\"conv\"](\n                x=x,\n                edge_index=edge_index,\n                edge_attr=edge_attr,\n                batch=batch,\n            )\n            x = layer[\"norm\"](x)\n\n        x = self.pre_decoder_norm(x)\n        x = self.decoder(x)\n\n        return x\n</code></pre> <code>forward(x, pe, edge_index, edge_attr, batch)</code> \u00b6 <p>Forward pass for the GPSTransformer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input node features of shape [num_nodes, input_dim].</p> required <code>pe</code> <code>Tensor</code> <p>Positional encoding of shape [num_nodes, pe_dim].</p> required <code>edge_index</code> <code>Tensor</code> <p>Edge indices for graph convolution.</p> required <code>edge_attr</code> <code>Tensor</code> <p>Edge feature tensor.</p> required <code>batch</code> <code>Tensor</code> <p>Batch vector assigning nodes to graphs.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>Output node features of shape [num_nodes, output_dim].</p> Source code in <code>gridfm_graphkit/models/gps_transformer.py</code> <pre><code>def forward(self, x, pe, edge_index, edge_attr, batch):\n    \"\"\"\n    Forward pass for the GPSTransformer.\n\n    Args:\n        x (Tensor): Input node features of shape [num_nodes, input_dim].\n        pe (Tensor): Positional encoding of shape [num_nodes, pe_dim].\n        edge_index (Tensor): Edge indices for graph convolution.\n        edge_attr (Tensor): Edge feature tensor.\n        batch (Tensor): Batch vector assigning nodes to graphs.\n\n    Returns:\n        output (Tensor): Output node features of shape [num_nodes, output_dim].\n    \"\"\"\n    x_pe = self.pe_norm(pe)\n\n    x = self.encoder(x)\n    x = self.input_norm(x)\n\n    x = torch.cat((x, x_pe), 1)\n    for layer in self.layers:\n        x = layer[\"conv\"](\n            x=x,\n            edge_index=edge_index,\n            edge_attr=edge_attr,\n            batch=batch,\n        )\n        x = layer[\"norm\"](x)\n\n    x = self.pre_decoder_norm(x)\n    x = self.decoder(x)\n\n    return x\n</code></pre>"},{"location":"models/models/#gnn_transformerconv","title":"<code>GNN_TransformerConv</code>","text":"<p>               Bases: <code>Module</code></p> <p>Graph Neural Network using TransformerConv layers from PyTorch Geometric.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimensionality of input node features.</p> required <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size for TransformerConv layers.</p> required <code>output_dim</code> <code>int</code> <p>Output dimension size.</p> required <code>edge_dim</code> <code>int</code> <p>Dimensionality of edge features.</p> required <code>num_layers</code> <code>int</code> <p>Number of TransformerConv layers.</p> required <code>heads</code> <code>int</code> <p>Number of attention heads.</p> <code>1</code> <code>mask_dim</code> <code>int</code> <p>Dimension of mask vector.</p> <code>6</code> <code>mask_value</code> <code>float</code> <p>Initial mask value.</p> <code>-1.0</code> <code>learn_mask</code> <code>bool</code> <p>Whether mask values are learnable.</p> <code>False</code> Source code in <code>gridfm_graphkit/models/graphTransformer.py</code> <pre><code>class GNN_TransformerConv(nn.Module):\n    \"\"\"\n    Graph Neural Network using [TransformerConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.TransformerConv.html) layers from PyTorch Geometric.\n\n    Args:\n        input_dim (int): Dimensionality of input node features.\n        hidden_dim (int): Hidden dimension size for TransformerConv layers.\n        output_dim (int): Output dimension size.\n        edge_dim (int): Dimensionality of edge features.\n        num_layers (int): Number of TransformerConv layers.\n        heads (int, optional): Number of attention heads.\n        mask_dim (int, optional): Dimension of mask vector.\n        mask_value (float, optional): Initial mask value.\n        learn_mask (bool, optional): Whether mask values are learnable.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int,\n        output_dim: int,\n        edge_dim: int,\n        num_layers: int,\n        heads: int = 1,\n        mask_dim: int = 6,\n        mask_value: float = -1.0,\n        learn_mask: bool = False,\n    ):\n        super(GNN_TransformerConv, self).__init__()\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.edge_dim = edge_dim\n        self.heads = heads\n        self.mask_dim = mask_dim\n        self.mask_value = mask_value\n        self.learn_mask = learn_mask\n\n        self.layers = nn.ModuleList()\n        current_dim = input_dim  # First layer takes `input_dim` as input\n\n        for _ in range(self.num_layers):\n            self.layers.append(\n                TransformerConv(\n                    current_dim,\n                    self.hidden_dim,\n                    heads=self.heads,\n                    edge_dim=self.edge_dim,\n                    beta=False,\n                ),\n            )\n            # Update the dimension for the next layer\n            current_dim = self.hidden_dim * self.heads\n\n        # Fully connected (MLP) layers after the GAT layers\n        self.mlps = nn.Sequential(\n            nn.Linear(self.hidden_dim * self.heads, self.hidden_dim),\n            nn.LeakyReLU(),\n            nn.Linear(self.hidden_dim, output_dim),\n        )\n\n        if learn_mask:\n            self.mask_value = nn.Parameter(\n                torch.randn(mask_dim) + mask_value,\n                requires_grad=True,\n            )\n        else:\n            self.mask_value = nn.Parameter(\n                torch.zeros(mask_dim) + mask_value,\n                requires_grad=False,\n            )\n\n    def forward(self, x, pe, edge_index, edge_attr, batch):\n        \"\"\"\n        Forward pass for the GPSTransformer.\n\n        Args:\n            x (Tensor): Input node features of shape [num_nodes, input_dim].\n            pe (Tensor): Positional encoding of shape [num_nodes, pe_dim] (not used).\n            edge_index (Tensor): Edge indices for graph convolution.\n            edge_attr (Tensor): Edge feature tensor.\n            batch (Tensor): Batch vector assigning nodes to graphs (not used).\n\n        Returns:\n            output (Tensor): Output node features of shape [num_nodes, output_dim].\n        \"\"\"\n        for conv in self.layers:\n            x = conv(x, edge_index, edge_attr)\n            x = nn.LeakyReLU()(x)\n\n        x = self.mlps(x)\n        return x\n</code></pre> <code>forward(x, pe, edge_index, edge_attr, batch)</code> \u00b6 <p>Forward pass for the GPSTransformer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input node features of shape [num_nodes, input_dim].</p> required <code>pe</code> <code>Tensor</code> <p>Positional encoding of shape [num_nodes, pe_dim] (not used).</p> required <code>edge_index</code> <code>Tensor</code> <p>Edge indices for graph convolution.</p> required <code>edge_attr</code> <code>Tensor</code> <p>Edge feature tensor.</p> required <code>batch</code> <code>Tensor</code> <p>Batch vector assigning nodes to graphs (not used).</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>Output node features of shape [num_nodes, output_dim].</p> Source code in <code>gridfm_graphkit/models/graphTransformer.py</code> <pre><code>def forward(self, x, pe, edge_index, edge_attr, batch):\n    \"\"\"\n    Forward pass for the GPSTransformer.\n\n    Args:\n        x (Tensor): Input node features of shape [num_nodes, input_dim].\n        pe (Tensor): Positional encoding of shape [num_nodes, pe_dim] (not used).\n        edge_index (Tensor): Edge indices for graph convolution.\n        edge_attr (Tensor): Edge feature tensor.\n        batch (Tensor): Batch vector assigning nodes to graphs (not used).\n\n    Returns:\n        output (Tensor): Output node features of shape [num_nodes, output_dim].\n    \"\"\"\n    for conv in self.layers:\n        x = conv(x, edge_index, edge_attr)\n        x = nn.LeakyReLU()(x)\n\n    x = self.mlps(x)\n    return x\n</code></pre>"},{"location":"quick_start/quick_start/","title":"CLI commands","text":"<p>An interface to train, fine-tune, and evaluate GridFM models using configurable YAML files and MLflow tracking.</p> <pre><code>gridfm_graphkit &lt;command&gt; [OPTIONS]\n</code></pre> <p>Available commands:</p> <ul> <li><code>train</code> \u2013 Train a new model</li> <li><code>predict</code> \u2013 Evaluate an existing model</li> <li><code>finetune</code> \u2013 Fine-tune a pre-trained model</li> </ul>"},{"location":"quick_start/quick_start/#training-models","title":"Training Models","text":"<pre><code>gridfm_graphkit train --config path/to/config.yaml\n</code></pre>"},{"location":"quick_start/quick_start/#arguments","title":"Arguments","text":"Argument Type Description Default <code>--config</code> <code>str</code> Required for standard training. Path to base config YAML. <code>None</code> <code>--grid</code> <code>str</code> Optional. Path to grid search YAML. Not supported with <code>-c</code>. <code>None</code> <code>--exp</code> <code>str</code> Optional. MLflow experiment name. Defaults to a timestamp. <code>None</code> <code>--data_path</code> <code>str</code> Optional. Root dataset directory. <code>data</code> <code>-c</code> <code>flag</code> Optional. Enable checkpoint mode. <code>False</code> <code>--model_exp_id</code> <code>str</code> Required if <code>-c</code> is used. MLflow experiment ID. <code>None</code> <code>--model_run_id</code> <code>str</code> Required if <code>-c</code> is used. MLflow run ID. <code>None</code>"},{"location":"quick_start/quick_start/#examples","title":"Examples","text":"<p>Standard Training:</p> <pre><code>gridfm_graphkit train --config config/train.yaml --exp \"run1\"\n</code></pre> <p>Grid Search Training:</p> <pre><code>gridfm_graphkit train --config config/train.yaml --grid config/grid.yaml\n</code></pre> <p>Training from Checkpoint:</p> <pre><code>gridfm_graphkit train -c --model_exp_id 123 --model_run_id abc\n</code></pre>"},{"location":"quick_start/quick_start/#evaluating-models","title":"Evaluating Models","text":"<pre><code>gridfm_graphkit predict --model_path model.pth --config config/eval.yaml --eval_name run_eval\n</code></pre>"},{"location":"quick_start/quick_start/#arguments_1","title":"Arguments","text":"Argument Type Description Default <code>--model_path</code> <code>str</code> Optional. Path to a model file. <code>None</code> <code>--model_exp_id</code> <code>str</code> Required if <code>--model_path</code> is not used. MLflow experiment ID. <code>None</code> <code>--model_run_id</code> <code>str</code> Required if <code>--model_path</code> is not used. MLflow run ID. <code>None</code> <code>--model_name</code> <code>str</code> Optional. Filename inside MLflow artifacts. <code>best_model</code> <code>--config</code> <code>str</code> Required. Path to evaluation config. <code>None</code> <code>--eval_name</code> <code>str</code> Required. Name of the evaluation run in MLflow. <code>None</code> <code>--data_path</code> <code>str</code> Optional. Path to dataset directory. <code>data</code>"},{"location":"quick_start/quick_start/#examples_1","title":"Examples","text":"<p>Evaluate a Logged MLflow Model:</p> <pre><code>gridfm_graphkit predict --config config/eval.yaml --eval_name run_eval --model_exp_id 1 --model_run_id abc\n</code></pre>"},{"location":"quick_start/quick_start/#fine-tuning-models","title":"Fine-Tuning Models","text":"<pre><code>gridfm_graphkit finetune --config path/to/config.yaml --model_path path/to/model.pth\n</code></pre>"},{"location":"quick_start/quick_start/#arguments_2","title":"Arguments","text":"Argument Type Description Default <code>--config</code> <code>str</code> Required. Fine-tuning configuration file. <code>None</code> <code>--model_path</code> <code>str</code> Required. Path to a pre-trained model file. <code>None</code> <code>--exp</code> <code>str</code> Optional. MLflow experiment name. <code>None</code> <code>--data_path</code> <code>str</code> Optional. Root dataset directory. <code>data</code>"},{"location":"quick_start/yaml_config/","title":"Configuration Parameters","text":"<p>Detailed explanation of the parameters used in the YAML configuration file. The configuration is structured into different sections for ease of use.</p>"},{"location":"quick_start/yaml_config/#data","title":"Data","text":"<ul> <li> <p><code>networks</code>: (List of Strings) Specifies the network topologies to use during training</p> <ul> <li>Example: <code>[\"case300_ieee\", \"case30_ieee\"]</code></li> </ul> </li> <li> <p><code>scenarios</code>: (List of Integers) Defines the number of scenarios to use for each network specified.</p> </li> <li> <p>Example: <code>[8500, 4000]</code></p> </li> <li> <p><code>normalization</code>: (String) Normalization method for data.</p> <ul> <li>Options:<ul> <li><code>minmax</code>: Scales data between the minimum and maximum values.</li> <li><code>standard</code>: Standardizes data to have zero mean and unit variance.</li> <li><code>baseMVAnorm</code>: Divides data by a baseMVA value, which is the maximum active/reactive power across the network.</li> <li><code>identity</code>: Leaves data unchanged.</li> </ul> </li> <li>Example: <code>\"baseMVAnorm\"</code></li> </ul> </li> <li> <p><code>baseMVA</code>: (Integer) The base MVA value specified in the original matpower casefile, needed for <code>baseMVAnorm</code> normalization.</p> <ul> <li>Default: <code>100</code></li> </ul> </li> <li> <p><code>mask_type</code>: (String) Masking strategy.</p> <ul> <li>Options:<ul> <li><code>rnd</code>: Mask each feature with a certain probability that needs to be specified (<code>mask_ratio</code>).</li> <li><code>pf</code>: Power flow problem setup.</li> <li><code>opf</code>: Optimal power flow problem setup.</li> <li><code>none</code>: No masking.</li> </ul> </li> <li>Example: <code>\"rnd\"</code></li> </ul> </li> <li> <p><code>mask_value</code>: (Float) Value used to mask data during training.</p> <ul> <li>Default: <code>0.0</code></li> </ul> </li> <li> <p><code>mask_ratio</code>: (Float) Propability of each feature to be masked, needs to be specified only when mask_type is <code>rnd</code>.</p> <ul> <li>Default: <code>0.5</code></li> </ul> </li> <li> <p><code>mask_dim</code>: (Integer) Number of features to mask.</p> <ul> <li>Default: <code>6</code> (Pd, Qd, Pg, Qg, Vm, Va)</li> </ul> </li> <li> <p><code>learn_mask</code>: (Boolean) Specifies whether the mask value is learnable.</p> <ul> <li>Default: <code>False</code></li> </ul> </li> <li> <p><code>val_ratio</code>: (Float) Fraction of data used for validation.</p> <ul> <li>Default: <code>0.1</code></li> </ul> </li> <li> <p><code>test_ratio</code>: (Float) Fraction of data used for testing.</p> <ul> <li>Default: <code>0.1</code></li> </ul> </li> </ul>"},{"location":"quick_start/yaml_config/#model","title":"Model","text":"<ul> <li> <p><code>type</code>: (String) Specifies the type of model architecture.</p> <ul> <li>Example: <code>\"GPSconv\"</code></li> </ul> </li> <li> <p><code>input_dim</code>: (Integer) Input dimensionality of the model.</p> <ul> <li>Default: <code>9</code> (Pd, Qd, Pg, Qg, Vm, Va, PQ, PV, REF)</li> </ul> </li> <li> <p><code>output_dim</code>: (Integer) Output dimensionality of the model.</p> <ul> <li>Default: <code>6</code> (Pd, Qd, Pg, Qg, Vm, Va)</li> </ul> </li> <li> <p><code>edge_dim</code>: (Integer) Dimensionality of edge features.</p> <ul> <li>Default: <code>2</code> (G, B)</li> </ul> </li> <li> <p><code>pe_dim</code>: (Integer) Dimensionality of positional encoding.</p> <ul> <li>Example: <code>20</code> (Length of random walk)</li> </ul> </li> <li> <p><code>num_layers</code>: (Integer) Number of layers in the model.</p> <ul> <li>Example: <code>6</code></li> </ul> </li> <li> <p><code>hidden_size</code>: (Integer) Size of hidden layers.</p> <ul> <li>Example: <code>256</code></li> </ul> </li> <li> <p><code>attention_head</code>: (Integer) Number of attention heads in the model.</p> <ul> <li>Example: <code>8</code></li> </ul> </li> <li> <p><code>dropout</code>: (Float) Model dropout probability</p> <ul> <li>Default: <code>0.0</code></li> </ul> </li> </ul>"},{"location":"quick_start/yaml_config/#training","title":"Training","text":"<ul> <li> <p><code>batch_size</code>: (Integer) Number of samples per training batch.</p> <ul> <li>Example: <code>16</code></li> </ul> </li> <li> <p><code>epochs</code>: (Integer) Number of training epochs.</p> <ul> <li>Example: <code>100</code></li> </ul> </li> <li> <p><code>losses</code>: (List of Strings) Specifies the loss functions to use during training.</p> <ul> <li>Available options:<ul> <li><code>MSE</code>: Mean Squared Error.</li> <li><code>MaskedMSE</code>: Masked Mean Squared Error.</li> <li><code>SCE</code>: Scaled Cosine Error.</li> <li><code>PBE</code>: Power Balance Equation loss.</li> </ul> </li> <li>Example: <code>[\"MaskedMSE\", \"PBE\"]</code></li> </ul> </li> <li> <p><code>loss_weights</code>: (List of Floats) Specifies the relative weights for each loss function.</p> <ul> <li>Example: <code>[0.01, 0.99]</code></li> </ul> </li> </ul>"},{"location":"quick_start/yaml_config/#optimizer","title":"Optimizer","text":"<ul> <li> <p><code>learning_rate</code>: (Float) Learning rate for the optimizer.</p> <ul> <li>Example: <code>0.0001</code></li> </ul> </li> <li> <p><code>beta1</code>: (Float) Beta1 parameter for the Adam optimizer.</p> <ul> <li>Default: <code>0.9</code></li> </ul> </li> <li> <p><code>beta2</code>: (Float) Beta2 parameter for the Adam optimizer.</p> <ul> <li>Default: <code>0.999</code></li> </ul> </li> <li> <p><code>lr_decay</code>: (Float) Learning rate decay factor.     `</p> </li> <li> <p><code>lr_patience</code>: (Integer) Number of epochs to wait before applying learning rate decay.</p> <ul> <li>Example: <code>3</code></li> </ul> </li> </ul>"},{"location":"quick_start/yaml_config/#callbacks","title":"Callbacks","text":"<ul> <li> <p><code>patience</code>: (Integer) Number of epochs to wait before early stopping. A value of <code>-1</code> disables early stopping.</p> <ul> <li>Default: <code>-1</code></li> </ul> </li> <li> <p><code>tol</code>: (Float) Tolerance for validation loss comparison during early stopping.</p> <ul> <li>Default: <code>0</code></li> </ul> </li> </ul>"},{"location":"quick_start/yaml_config/#verbose","title":"Verbose","text":"<ul> <li><code>verbose</code>: (Boolean) Provides detailed analysis after training.<ul> <li>Default: <code>False</code></li> </ul> </li> </ul>"},{"location":"training/plugins/","title":"Trainer Plugins","text":"<p>Reusable plugins that hook into the training loop to perform tasks like logging or checkpointing.</p>"},{"location":"training/plugins/#trainerplugin","title":"<code>TrainerPlugin</code>","text":"<p>Base class for training plugins.</p> <p>A <code>TrainerPlugin</code> is invoked during the training process either at regular step intervals, at the end of each epoch, or both. It can be extended to perform actions like logging, checkpointing, or validation.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Interval (in steps) to run the plugin. If <code>None</code>, only runs at end of epoch</p> <code>None</code> Source code in <code>gridfm_graphkit/training/plugins.py</code> <pre><code>class TrainerPlugin:\n    \"\"\"\n    Base class for training plugins.\n\n    A `TrainerPlugin` is invoked during the training process either at regular step intervals,\n    at the end of each epoch, or both. It can be extended to perform actions like logging,\n    checkpointing, or validation.\n\n    Args:\n        steps (int, optional): Interval (in steps) to run the plugin. If `None`, only runs at end of epoch\n    \"\"\"\n\n    def __init__(self, steps: Optional[int] = None):\n        self.steps = steps\n\n    def run(self, step: int, end_of_epoch: bool) -&gt; bool:\n        \"\"\"\n        Determines whether to execute the plugin at the current step.\n\n        Args:\n            step (int): The current step number.\n            end_of_epoch (bool): Whether this is the end of the epoch.\n\n        Returns:\n            bool: True if the plugin should run; False otherwise.\n        \"\"\"\n        # By default we always run for epoch ends.\n        if end_of_epoch:\n            return True\n        # If self.steps is None, we're only recording epoch ends and this isn't one.\n        if self.steps is None:\n            return False\n        # record every `step` steps, starting from step `step`\n        if step != 0 and (step + 1) % self.steps == 0:\n            return True\n        return False\n\n    @abstractmethod\n    def step(\n        self,\n        epoch: int,\n        step: int,\n        metrics: Dict = {},\n        end_of_epoch: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        This method is called on every step of training, or with step=None\n        at the end of each epoch. Implementations can use the passed in\n        parameters for validation, checkpointing, logging, etc.\n\n        Args:\n            epoch (int): The current epoch number.\n            step (int): The current step within the epoch.\n            metrics (dict): Dictionary of training metrics (e.g., loss).\n            end_of_epoch (bool): Indicates if this call is at the end of an epoch.\n            **kwargs (Any): Additional parameters such as model, optimizer, scheduler.\n        \"\"\"\n        pass\n</code></pre> <code>run(step, end_of_epoch)</code> \u00b6 <p>Determines whether to execute the plugin at the current step.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>The current step number.</p> required <code>end_of_epoch</code> <code>bool</code> <p>Whether this is the end of the epoch.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the plugin should run; False otherwise.</p> Source code in <code>gridfm_graphkit/training/plugins.py</code> <pre><code>def run(self, step: int, end_of_epoch: bool) -&gt; bool:\n    \"\"\"\n    Determines whether to execute the plugin at the current step.\n\n    Args:\n        step (int): The current step number.\n        end_of_epoch (bool): Whether this is the end of the epoch.\n\n    Returns:\n        bool: True if the plugin should run; False otherwise.\n    \"\"\"\n    # By default we always run for epoch ends.\n    if end_of_epoch:\n        return True\n    # If self.steps is None, we're only recording epoch ends and this isn't one.\n    if self.steps is None:\n        return False\n    # record every `step` steps, starting from step `step`\n    if step != 0 and (step + 1) % self.steps == 0:\n        return True\n    return False\n</code></pre> <code>step(epoch, step, metrics={}, end_of_epoch=False, **kwargs)</code> <code>abstractmethod</code> \u00b6 <p>This method is called on every step of training, or with step=None at the end of each epoch. Implementations can use the passed in parameters for validation, checkpointing, logging, etc.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>The current epoch number.</p> required <code>step</code> <code>int</code> <p>The current step within the epoch.</p> required <code>metrics</code> <code>dict</code> <p>Dictionary of training metrics (e.g., loss).</p> <code>{}</code> <code>end_of_epoch</code> <code>bool</code> <p>Indicates if this call is at the end of an epoch.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters such as model, optimizer, scheduler.</p> <code>{}</code> Source code in <code>gridfm_graphkit/training/plugins.py</code> <pre><code>@abstractmethod\ndef step(\n    self,\n    epoch: int,\n    step: int,\n    metrics: Dict = {},\n    end_of_epoch: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    This method is called on every step of training, or with step=None\n    at the end of each epoch. Implementations can use the passed in\n    parameters for validation, checkpointing, logging, etc.\n\n    Args:\n        epoch (int): The current epoch number.\n        step (int): The current step within the epoch.\n        metrics (dict): Dictionary of training metrics (e.g., loss).\n        end_of_epoch (bool): Indicates if this call is at the end of an epoch.\n        **kwargs (Any): Additional parameters such as model, optimizer, scheduler.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"training/plugins/#mlflowloggerplugin","title":"<code>MLflowLoggerPlugin</code>","text":"<p>               Bases: <code>TrainerPlugin</code></p> <p>Plugin to log training metrics to MLflow.</p> <p>Logs metrics dynamically during training at defined step intervals and/or at the end of each epoch. Also logs initial training parameters once.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Interval in steps to log metrics.</p> <code>None</code> <code>params</code> <code>dict</code> <p>Parameters to log to MLflow at the start.</p> <code>None</code> Source code in <code>gridfm_graphkit/training/plugins.py</code> <pre><code>class MLflowLoggerPlugin(TrainerPlugin):\n    \"\"\"\n    Plugin to log training metrics to MLflow.\n\n    Logs metrics dynamically during training at defined step intervals and/or\n    at the end of each epoch. Also logs initial training parameters once.\n\n    Args:\n        steps (int, optional): Interval in steps to log metrics.\n        params (dict, optional): Parameters to log to MLflow at the start.\n    \"\"\"\n\n    def __init__(self, steps: Optional[int] = None, params: dict = None):\n        super().__init__(steps=steps)  # Initialize the steps from the base class\n        self.steps = steps\n        self.metrics_history = {}  # Dictionary to hold lists of all metrics over time\n        if params:\n            # Log parameters to MLflow at the beginning of training\n            mlflow.log_params(params)\n\n    def step(\n        self,\n        epoch: int,\n        step: int,\n        metrics: Dict = {},\n        end_of_epoch: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Logs metrics to MLflow dynamically at each specified step and at the end of each epoch.\n\n        Args:\n            epoch (int): The current epoch number.\n            step (int): The current step within the epoch.\n            metrics (Dict): Dictionary of metrics to log, e.g., {'train_loss': value}.\n            end_of_epoch (bool): Flag indicating whether this is the end of the epoch.\n        \"\"\"\n        for metric_name, metric_value in metrics.items():\n            # Add metric to history\n            if metric_name not in self.metrics_history:\n                self.metrics_history[metric_name] = []\n            self.metrics_history[metric_name].append(metric_value)\n\n        if end_of_epoch:\n            for metric_name, values in self.metrics_history.items():\n                if values:  # Avoid division by zero or empty lists\n                    avg_value = sum(values) / len(values)\n                    mlflow.log_metric(f\"{metric_name}\", avg_value, step=epoch)\n\n            # Clear metrics for the next epoch\n            self.metrics_history = {}\n</code></pre> <code>step(epoch, step, metrics={}, end_of_epoch=False, **kwargs)</code> \u00b6 <p>Logs metrics to MLflow dynamically at each specified step and at the end of each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>The current epoch number.</p> required <code>step</code> <code>int</code> <p>The current step within the epoch.</p> required <code>metrics</code> <code>Dict</code> <p>Dictionary of metrics to log, e.g., {'train_loss': value}.</p> <code>{}</code> <code>end_of_epoch</code> <code>bool</code> <p>Flag indicating whether this is the end of the epoch.</p> <code>False</code> Source code in <code>gridfm_graphkit/training/plugins.py</code> <pre><code>def step(\n    self,\n    epoch: int,\n    step: int,\n    metrics: Dict = {},\n    end_of_epoch: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Logs metrics to MLflow dynamically at each specified step and at the end of each epoch.\n\n    Args:\n        epoch (int): The current epoch number.\n        step (int): The current step within the epoch.\n        metrics (Dict): Dictionary of metrics to log, e.g., {'train_loss': value}.\n        end_of_epoch (bool): Flag indicating whether this is the end of the epoch.\n    \"\"\"\n    for metric_name, metric_value in metrics.items():\n        # Add metric to history\n        if metric_name not in self.metrics_history:\n            self.metrics_history[metric_name] = []\n        self.metrics_history[metric_name].append(metric_value)\n\n    if end_of_epoch:\n        for metric_name, values in self.metrics_history.items():\n            if values:  # Avoid division by zero or empty lists\n                avg_value = sum(values) / len(values)\n                mlflow.log_metric(f\"{metric_name}\", avg_value, step=epoch)\n\n        # Clear metrics for the next epoch\n        self.metrics_history = {}\n</code></pre>"},{"location":"training/plugins/#checkpointerplugin","title":"<code>CheckpointerPlugin</code>","text":"<p>               Bases: <code>TrainerPlugin</code></p> <p>Plugin to periodically save model checkpoints.</p> <p>Stores the model, optimizer, and scheduler states to a given directory at specified step intervals or at the end of each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>Directory where checkpoints will be saved.</p> required <code>steps</code> <code>int</code> <p>Interval in steps for checkpointing.</p> <code>None</code> Source code in <code>gridfm_graphkit/training/plugins.py</code> <pre><code>class CheckpointerPlugin(TrainerPlugin):\n    \"\"\"\n    Plugin to periodically save model checkpoints.\n\n    Stores the model, optimizer, and scheduler states to a given directory\n    at specified step intervals or at the end of each epoch.\n\n    Args:\n        checkpoint_dir (str): Directory where checkpoints will be saved.\n        steps (int, optional): Interval in steps for checkpointing.\n    \"\"\"\n\n    def __init__(\n        self,\n        checkpoint_dir: str,\n        steps: Optional[int] = None,\n    ):\n        super().__init__(steps=steps)\n        self.checkpoint_dir = checkpoint_dir\n        os.makedirs(self.checkpoint_dir, exist_ok=True)\n\n    def step(\n        self,\n        epoch: int,\n        step: int,\n        metrics: Dict = {},\n        end_of_epoch: bool = False,\n        model: Optional[nn.Module] = None,\n        optimizer: Optional[Optimizer] = None,\n        scheduler: Optional[LRScheduler] = None,\n    ):\n        \"\"\"\n        Saves a checkpoint if the conditions to run the plugin are met.\n\n        Args:\n            epoch (int): Current epoch number.\n            step (int): Current training step.\n            metrics (dict): Optional metrics dictionary (unused here).\n            end_of_epoch (bool): Whether this is the end of the epoch.\n            model (nn.Module, optional): Model to be checkpointed.\n            optimizer (Optimizer, optional): Optimizer to save.\n            scheduler (LRScheduler, optional): Scheduler to save.\n        \"\"\"\n        # Check if we should save at this step or end of epoch\n        if not self.run(step, end_of_epoch):\n            return\n\n        checkpoint = {\n            \"epoch\": epoch,\n            \"model_state_dict\": model.state_dict() if model else None,\n            \"optimizer_state_dict\": optimizer.state_dict() if optimizer else None,\n            \"scheduler_state_dict\": scheduler.state_dict() if scheduler else None,\n        }\n\n        checkpoint_path = os.path.join(\n            self.checkpoint_dir,\n            \"checkpoint_last_epoch.pth\",\n        )\n        torch.save(checkpoint, checkpoint_path)\n</code></pre> <code>step(epoch, step, metrics={}, end_of_epoch=False, model=None, optimizer=None, scheduler=None)</code> \u00b6 <p>Saves a checkpoint if the conditions to run the plugin are met.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Current epoch number.</p> required <code>step</code> <code>int</code> <p>Current training step.</p> required <code>metrics</code> <code>dict</code> <p>Optional metrics dictionary (unused here).</p> <code>{}</code> <code>end_of_epoch</code> <code>bool</code> <p>Whether this is the end of the epoch.</p> <code>False</code> <code>model</code> <code>Module</code> <p>Model to be checkpointed.</p> <code>None</code> <code>optimizer</code> <code>Optimizer</code> <p>Optimizer to save.</p> <code>None</code> <code>scheduler</code> <code>LRScheduler</code> <p>Scheduler to save.</p> <code>None</code> Source code in <code>gridfm_graphkit/training/plugins.py</code> <pre><code>def step(\n    self,\n    epoch: int,\n    step: int,\n    metrics: Dict = {},\n    end_of_epoch: bool = False,\n    model: Optional[nn.Module] = None,\n    optimizer: Optional[Optimizer] = None,\n    scheduler: Optional[LRScheduler] = None,\n):\n    \"\"\"\n    Saves a checkpoint if the conditions to run the plugin are met.\n\n    Args:\n        epoch (int): Current epoch number.\n        step (int): Current training step.\n        metrics (dict): Optional metrics dictionary (unused here).\n        end_of_epoch (bool): Whether this is the end of the epoch.\n        model (nn.Module, optional): Model to be checkpointed.\n        optimizer (Optimizer, optional): Optimizer to save.\n        scheduler (LRScheduler, optional): Scheduler to save.\n    \"\"\"\n    # Check if we should save at this step or end of epoch\n    if not self.run(step, end_of_epoch):\n        return\n\n    checkpoint = {\n        \"epoch\": epoch,\n        \"model_state_dict\": model.state_dict() if model else None,\n        \"optimizer_state_dict\": optimizer.state_dict() if optimizer else None,\n        \"scheduler_state_dict\": scheduler.state_dict() if scheduler else None,\n    }\n\n    checkpoint_path = os.path.join(\n        self.checkpoint_dir,\n        \"checkpoint_last_epoch.pth\",\n    )\n    torch.save(checkpoint, checkpoint_path)\n</code></pre>"},{"location":"training/plugins/#metricstrackerplugin","title":"<code>MetricsTrackerPlugin</code>","text":"<p>               Bases: <code>TrainerPlugin</code></p> <p>Logs metrics at the end of each epoch. Currently only returning the validation loss.</p> Source code in <code>gridfm_graphkit/training/plugins.py</code> <pre><code>class MetricsTrackerPlugin(TrainerPlugin):\n    \"\"\"\n    Logs metrics at the end of each epoch. Currently only returning the validation loss.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.validation_losses = []\n        self.metrics_history = {}\n\n    def step(\n        self,\n        epoch: int,\n        step: int,\n        metrics: Dict = {},\n        end_of_epoch: bool = False,\n        **kwargs,\n    ):\n        for metric_name, metric_value in metrics.items():\n            # Add metric to history\n            if metric_name not in self.metrics_history:\n                self.metrics_history[metric_name] = []\n            self.metrics_history[metric_name].append(metric_value)\n\n        if end_of_epoch:\n            for metric_name, values in self.metrics_history.items():\n                if values:  # Avoid division by zero or empty lists\n                    avg_value = sum(values) / len(values)\n                    if metric_name == \"Validation Loss\":\n                        self.validation_losses.append(avg_value)\n\n    def get_losses(self):\n        return self.validation_losses\n</code></pre>"},{"location":"training/trainer/","title":"Trainer","text":"<p>A flexible, modular training loop designed for GNN models in the GridFM framework. Handles training, validation, early stopping, learning rate scheduling, and plugin callbacks.</p>"},{"location":"training/trainer/#trainer_1","title":"<code>Trainer</code>","text":"<p>A flexible training loop for GridFM models with optional validation, learning rate scheduling, and plugin callbacks for logging or custom behavior.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Module</code> <p>The PyTorch model to train.</p> <code>optimizer</code> <code>Optimizer</code> <p>The optimizer used for updating model parameters.</p> <code>device</code> <p>The device to train on (CPU or CUDA).</p> <code>loss_fn</code> <code>Module</code> <p>Loss function that returns a loss dictionary.</p> <code>early_stopper</code> <code>EarlyStopper</code> <p>Callback for early stopping based on validation loss.</p> <code>train_dataloader</code> <code>DataLoader</code> <p>Dataloader for training data.</p> <code>val_dataloader</code> <code>DataLoader</code> <p>Dataloader for validation data.</p> <code>lr_scheduler</code> <code>optional</code> <p>Learning rate scheduler.</p> <code>plugins</code> <code>List[TrainerPlugin]</code> <p>List of plugin callbacks.</p> Source code in <code>gridfm_graphkit/training/trainer.py</code> <pre><code>class Trainer:\n    \"\"\"\n    A flexible training loop for GridFM models with optional validation, learning rate scheduling,\n    and plugin callbacks for logging or custom behavior.\n\n    Attributes:\n        model (nn.Module): The PyTorch model to train.\n        optimizer (Optimizer): The optimizer used for updating model parameters.\n        device: The device to train on (CPU or CUDA).\n        loss_fn (nn.Module): Loss function that returns a loss dictionary.\n        early_stopper (EarlyStopper): Callback for early stopping based on validation loss.\n        train_dataloader (DataLoader): Dataloader for training data.\n        val_dataloader (DataLoader, optional): Dataloader for validation data.\n        lr_scheduler (optional): Learning rate scheduler.\n        plugins (List[TrainerPlugin]): List of plugin callbacks.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        optimizer: Optimizer,\n        device,\n        loss_fn: nn.Module,\n        early_stopper: EarlyStopper,\n        train_dataloader: DataLoader,\n        val_dataloader: DataLoader,\n        lr_scheduler=None,\n        plugins: List[TrainerPlugin] = [],\n    ):\n        self.model = model\n        self.optimizer = optimizer\n        self.device = device\n        self.early_stopper = early_stopper\n        self.loss_fn = loss_fn\n        self.train_dataloader = train_dataloader\n        self.val_dataloader = val_dataloader\n        self.lr_scheduler = lr_scheduler\n        self.plugins = plugins\n\n    def __one_step(\n        self,\n        input: torch.Tensor,\n        edge_index: torch.Tensor,\n        label: torch.Tensor,\n        edge_attr: torch.Tensor,\n        mask: torch.Tensor = None,\n        batch: torch.Tensor = None,\n        pe: torch.Tensor = None,\n        val: bool = False,\n    ):\n        # expand the learnable mask to the input shape\n        mask_value_expanded = self.model.mask_value.expand(input.shape[0], -1)\n        # The line below will overwrite the last mask values, which is fine as long as the features which are masked do not change between batches\n        # set the learnable mask to the inout where it should be masked\n        input[:, : mask.shape[1]][mask] = mask_value_expanded[mask]\n        output = self.model(input, pe, edge_index, edge_attr, batch)\n\n        loss_dict = self.loss_fn(output, label, edge_index, edge_attr, mask)\n\n        if not val:\n            self.optimizer.zero_grad()\n            loss_dict[\"loss\"].backward()\n            self.optimizer.step()\n\n        return loss_dict\n\n    def __one_epoch(self, epoch: int, prev_step: int):\n        self.model.train()\n\n        highest_step = prev_step\n        for step, batch in enumerate(self.train_dataloader):\n            step = prev_step + step + 1\n            highest_step = step\n            batch = batch.to(self.device)\n\n            mask = getattr(batch, \"mask\", None)\n\n            loss_dict = self.__one_step(\n                batch.x,\n                batch.edge_index,\n                batch.y,\n                batch.edge_attr,\n                mask,\n                batch.batch,\n                batch.pe,\n            )\n            current_lr = self.optimizer.param_groups[0][\"lr\"]\n            metrics = {}\n            metrics[\"Training Loss\"] = loss_dict[\"loss\"].item()\n            metrics[\"Learning Rate\"] = current_lr\n\n            if self.model.learn_mask:\n                metrics[\"Mask Gradient Norm\"] = self.model.mask_value.grad.norm().item()\n\n            for plugin in self.plugins:\n                plugin.step(epoch, step, metrics=metrics)\n\n        self.model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for batch in self.val_dataloader:\n                batch = batch.to(self.device)\n                mask = getattr(batch, \"mask\", None)\n                metrics = self.__one_step(\n                    batch.x,\n                    batch.edge_index,\n                    batch.y,\n                    batch.edge_attr,\n                    mask,\n                    batch.batch,\n                    batch.pe,\n                    True,\n                )\n                val_loss += metrics[\"loss\"].item()\n                metrics[\"Validation Loss\"] = metrics.pop(\"loss\").item()\n\n                for plugin in self.plugins:\n                    plugin.step(epoch, step, metrics=metrics)\n        val_loss /= len(self.val_dataloader)\n        if self.lr_scheduler is not None:\n            self.lr_scheduler.step(val_loss)\n        for plugin in self.plugins:\n            plugin.step(\n                epoch,\n                step=highest_step,\n                end_of_epoch=True,\n                model=self.model,\n                optimizer=self.optimizer,\n                scheduler=self.lr_scheduler,\n            )\n        return val_loss\n\n    def train(self, start_epoch: int = 0, epochs: int = 1, prev_step: int = -1):\n        \"\"\"\n        Main training loop.\n\n        Args:\n            start_epoch (int): Epoch to start training from.\n            epochs (int): Total number of epochs to train.\n            prev_step (int): Previous training step (for logging continuity).\n        \"\"\"\n        for epoch in tqdm(range(start_epoch, start_epoch + epochs), desc=\"Epochs\"):\n            val_loss = self.__one_epoch(epoch, prev_step)\n            if self.early_stopper.early_stop(val_loss, self.model):\n                break\n</code></pre> <code>train(start_epoch=0, epochs=1, prev_step=-1)</code> \u00b6 <p>Main training loop.</p> <p>Parameters:</p> Name Type Description Default <code>start_epoch</code> <code>int</code> <p>Epoch to start training from.</p> <code>0</code> <code>epochs</code> <code>int</code> <p>Total number of epochs to train.</p> <code>1</code> <code>prev_step</code> <code>int</code> <p>Previous training step (for logging continuity).</p> <code>-1</code> Source code in <code>gridfm_graphkit/training/trainer.py</code> <pre><code>def train(self, start_epoch: int = 0, epochs: int = 1, prev_step: int = -1):\n    \"\"\"\n    Main training loop.\n\n    Args:\n        start_epoch (int): Epoch to start training from.\n        epochs (int): Total number of epochs to train.\n        prev_step (int): Previous training step (for logging continuity).\n    \"\"\"\n    for epoch in tqdm(range(start_epoch, start_epoch + epochs), desc=\"Epochs\"):\n        val_loss = self.__one_epoch(epoch, prev_step)\n        if self.early_stopper.early_stop(val_loss, self.model):\n            break\n</code></pre>"},{"location":"training/trainer/#usage-example","title":"Usage Example","text":"<pre><code>from gridfm_graphkit.training.trainer import Trainer\nfrom gridfm_graphkit.training.callbacks import EarlyStopper\nfrom gridfm_graphkit.training.plugins import MLflowLoggerPlugin\n\ntrainer = Trainer(\n    model=model,\n    optimizer=optimizer,\n    device=device,\n    loss_fn=loss_function,\n    early_stopper=EarlyStopper(save_path),\n    train_dataloader=train_loader,\n    val_dataloader=val_loader,\n    lr_scheduler=scheduler,\n    plugins=[MLflowLoggerPlugin()]\n)\n\ntrainer.train(start_epoch=0, epochs=100)\n</code></pre>"},{"location":"utils/loss/","title":"Loss Functions","text":""},{"location":"utils/loss/#power-balance-equation-loss","title":"<code>Power Balance Equation Loss</code>","text":"\\[ \\mathcal{L}_{\\text{PBE}} = \\frac{1}{N} \\sum_{i=1}^N \\left| (P_{G,i} - P_{D,i}) + j(Q_{G,i} - Q_{D,i}) - S_{\\text{injection}, i} \\right| \\] <p>               Bases: <code>Module</code></p> <p>Loss based on the Power Balance Equations.</p> Source code in <code>gridfm_graphkit/utils/loss.py</code> <pre><code>class PBELoss(nn.Module):\n    \"\"\"\n    Loss based on the Power Balance Equations.\n    \"\"\"\n\n    def __init__(self, visualization=False):\n        super(PBELoss, self).__init__()\n\n        self.visualization = visualization\n\n    def forward(self, pred, target, edge_index, edge_attr, mask):\n        # Create a temporary copy of pred to avoid modifying it\n        temp_pred = pred.clone()\n\n        # If a value is not masked, then use the original one\n        unmasked = ~mask\n        temp_pred[unmasked] = target[unmasked]\n\n        # Voltage magnitudes and angles\n        V_m = temp_pred[:, VM]  # Voltage magnitudes\n        V_a = temp_pred[:, VA]  # Voltage angles\n\n        # Compute the complex voltage vector V\n        V = V_m * torch.exp(1j * V_a)\n\n        # Compute the conjugate of V\n        V_conj = torch.conj(V)\n\n        # Extract edge attributes for Y_bus\n        edge_complex = edge_attr[:, G] + 1j * edge_attr[:, B]\n\n        # Construct sparse admittance matrix (real and imaginary parts separately)\n        Y_bus_sparse = to_torch_coo_tensor(\n            edge_index,\n            edge_complex,\n            size=(target.size(0), target.size(0)),\n        )\n\n        # Conjugate of the admittance matrix\n        Y_bus_conj = torch.conj(Y_bus_sparse)\n\n        # Compute the complex power injection S_injection\n        S_injection = torch.diag(V) @ Y_bus_conj @ V_conj\n\n        # Compute net power balance\n        net_P = temp_pred[:, PG] - temp_pred[:, PD]\n        net_Q = temp_pred[:, QG] - temp_pred[:, QD]\n        S_net_power_balance = net_P + 1j * net_Q\n\n        # Power balance loss\n        loss = torch.mean(\n            torch.abs(S_net_power_balance - S_injection),\n        )  # Mean of absolute complex power value\n\n        real_loss_power = torch.mean(\n            torch.abs(torch.real(S_net_power_balance - S_injection)),\n        )\n        imag_loss_power = torch.mean(\n            torch.abs(torch.imag(S_net_power_balance - S_injection)),\n        )\n        if self.visualization:\n            return {\n                \"loss\": loss,\n                \"Power power loss in p.u.\": loss.item(),\n                \"Active Power Loss in p.u.\": real_loss_power.item(),\n                \"Reactive Power Loss in p.u.\": imag_loss_power.item(),\n                \"Nodal Active Power Loss in p.u.\": torch.abs(\n                    torch.real(S_net_power_balance - S_injection),\n                ),\n                \"Nodal Reactive Power Loss in p.u.\": torch.abs(\n                    torch.imag(S_net_power_balance - S_injection),\n                ),\n            }\n        else:\n            return {\n                \"loss\": loss,\n                \"Power power loss in p.u.\": loss.item(),\n                \"Active Power Loss in p.u.\": real_loss_power.item(),\n                \"Reactive Power Loss in p.u.\": imag_loss_power.item(),\n            }\n</code></pre>"},{"location":"utils/loss/#mean-squared-error-loss","title":"<code>Mean Squared Error Loss</code>","text":"\\[ \\mathcal{L}_{\\text{MSE}} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 \\] <p>               Bases: <code>Module</code></p> <p>Standard Mean Squared Error loss.</p> Source code in <code>gridfm_graphkit/utils/loss.py</code> <pre><code>class MSELoss(nn.Module):\n    \"\"\"Standard Mean Squared Error loss.\"\"\"\n\n    def __init__(self, reduction=\"mean\"):\n        super(MSELoss, self).__init__()\n        self.reduction = reduction\n\n    def forward(self, pred, target, edge_index=None, edge_attr=None, mask=None):\n        loss = F.mse_loss(pred, target, reduction=self.reduction)\n        return {\"loss\": loss, \"MSE loss\": loss.item()}\n</code></pre>"},{"location":"utils/loss/#masked-mean-squared-error-loss","title":"<code>Masked Mean Squared Error Loss</code>","text":"\\[ \\mathcal{L}_{\\text{MaskedMSE}} = \\frac{1}{|M|} \\sum_{i \\in M} (y_i - \\hat{y}_i)^2 \\] <p>               Bases: <code>Module</code></p> <p>Mean Squared Error loss computed only on masked elements.</p> Source code in <code>gridfm_graphkit/utils/loss.py</code> <pre><code>class MaskedMSELoss(nn.Module):\n    \"\"\"\n    Mean Squared Error loss computed only on masked elements.\n    \"\"\"\n\n    def __init__(self, reduction=\"mean\"):\n        super(MaskedMSELoss, self).__init__()\n        self.reduction = reduction\n\n    def forward(self, pred, target, edge_index=None, edge_attr=None, mask=None):\n        loss = F.mse_loss(pred[mask], target[mask], reduction=self.reduction)\n        return {\"loss\": loss, \"Masked MSE loss\": loss.item()}\n</code></pre>"},{"location":"utils/loss/#scaled-cosine-error-loss","title":"<code>Scaled Cosine Error Loss</code>","text":"\\[ \\mathcal{L}_{\\text{SCE}} = \\frac{1}{N} \\sum_{i=1}^N \\left(1 - \\frac{\\hat{y}^T_i \\cdot y_i}{\\|\\hat{y}_i\\| \\|y_i\\|}\\right)^\\alpha \\text{ , } \\alpha \\geq 1 \\] <p>               Bases: <code>Module</code></p> <p>Scaled Cosine Error Loss with optional masking and normalization.</p> Source code in <code>gridfm_graphkit/utils/loss.py</code> <pre><code>class SCELoss(nn.Module):\n    \"\"\"Scaled Cosine Error Loss with optional masking and normalization.\"\"\"\n\n    def __init__(self, alpha=3):\n        super(SCELoss, self).__init__()\n        self.alpha = alpha\n\n    def forward(self, pred, target, edge_index=None, edge_attr=None, mask=None):\n        if mask is not None:\n            pred = F.normalize(pred[mask], p=2, dim=-1)\n            target = F.normalize(target[mask], p=2, dim=-1)\n        else:\n            pred = F.normalize(pred, p=2, dim=-1)\n            target = F.normalize(target, p=2, dim=-1)\n\n        loss = ((1 - (pred * target).sum(dim=-1)).pow(self.alpha)).mean()\n\n        return {\n            \"loss\": loss,\n            \"SCE loss\": loss.item(),\n        }\n</code></pre>"},{"location":"utils/loss/#mixed-loss","title":"<code>Mixed Loss</code>","text":"\\[ \\mathcal{L}_{\\text{Mixed}} = \\sum_{m=1}^M w_m \\cdot \\mathcal{L}_m \\] <p>               Bases: <code>Module</code></p> <p>Combines multiple loss functions with weighted sum.</p> <p>Parameters:</p> Name Type Description Default <code>loss_functions</code> <code>list[Module]</code> <p>List of loss functions.</p> required <code>weights</code> <code>list[float]</code> <p>Corresponding weights for each loss function.</p> required Source code in <code>gridfm_graphkit/utils/loss.py</code> <pre><code>class MixedLoss(nn.Module):\n    \"\"\"\n    Combines multiple loss functions with weighted sum.\n\n    Args:\n        loss_functions (list[nn.Module]): List of loss functions.\n        weights (list[float]): Corresponding weights for each loss function.\n    \"\"\"\n\n    def __init__(self, loss_functions, weights):\n        super(MixedLoss, self).__init__()\n\n        if len(loss_functions) != len(weights):\n            raise ValueError(\n                \"The number of loss functions must match the number of weights.\",\n            )\n\n        self.loss_functions = nn.ModuleList(loss_functions)\n        self.weights = weights\n\n    def forward(self, pred, target, edge_index=None, edge_attr=None, mask=None):\n        \"\"\"\n        Compute the weighted sum of all specified losses.\n\n        Parameters:\n\n        - pred: Predictions.\n        - target: Ground truth.\n        - edge_index: Optional edge index for graph-based losses.\n        - edge_attr: Optional edge attributes for graph-based losses.\n        - mask: Optional mask to filter the inputs for certain losses.\n\n        Returns:\n        - A dictionary with the total loss and individual losses.\n        \"\"\"\n        total_loss = 0.0\n        loss_details = {}\n\n        for i, loss_fn in enumerate(self.loss_functions):\n            loss_output = loss_fn(\n                pred,\n                target,\n                edge_index=edge_index,\n                edge_attr=edge_attr,\n                mask=mask,\n            )\n\n            # Assume each loss function returns a dictionary with a \"loss\" key\n            individual_loss = loss_output.pop(\"loss\")\n            weighted_loss = self.weights[i] * individual_loss\n\n            total_loss += weighted_loss\n\n            # Add other keys from the loss output to the details\n            for key, val in loss_output.items():\n                loss_details[key] = val\n\n        loss_details[\"loss\"] = total_loss\n        return loss_details\n</code></pre> <code>forward(pred, target, edge_index=None, edge_attr=None, mask=None)</code> \u00b6 <p>Compute the weighted sum of all specified losses.</p> <p>Parameters:</p> <ul> <li>pred: Predictions.</li> <li>target: Ground truth.</li> <li>edge_index: Optional edge index for graph-based losses.</li> <li>edge_attr: Optional edge attributes for graph-based losses.</li> <li>mask: Optional mask to filter the inputs for certain losses.</li> </ul> <p>Returns: - A dictionary with the total loss and individual losses.</p> Source code in <code>gridfm_graphkit/utils/loss.py</code> <pre><code>def forward(self, pred, target, edge_index=None, edge_attr=None, mask=None):\n    \"\"\"\n    Compute the weighted sum of all specified losses.\n\n    Parameters:\n\n    - pred: Predictions.\n    - target: Ground truth.\n    - edge_index: Optional edge index for graph-based losses.\n    - edge_attr: Optional edge attributes for graph-based losses.\n    - mask: Optional mask to filter the inputs for certain losses.\n\n    Returns:\n    - A dictionary with the total loss and individual losses.\n    \"\"\"\n    total_loss = 0.0\n    loss_details = {}\n\n    for i, loss_fn in enumerate(self.loss_functions):\n        loss_output = loss_fn(\n            pred,\n            target,\n            edge_index=edge_index,\n            edge_attr=edge_attr,\n            mask=mask,\n        )\n\n        # Assume each loss function returns a dictionary with a \"loss\" key\n        individual_loss = loss_output.pop(\"loss\")\n        weighted_loss = self.weights[i] * individual_loss\n\n        total_loss += weighted_loss\n\n        # Add other keys from the loss output to the details\n        for key, val in loss_output.items():\n            loss_details[key] = val\n\n    loss_details[\"loss\"] = total_loss\n    return loss_details\n</code></pre>"}]}